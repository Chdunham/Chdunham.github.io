{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Encoder Decoder on Adult Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chdunham/Chdunham.github.io/blob/master/Encoder_Decoder_on_Adult_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZdcCNGS7YuH"
      },
      "source": [
        "In this post, we will see how to generate tabular synthetic data using Generative adversarial networks(GANs). The goal is to generate synthetic data that is similar to the actual data in terms of statistics and demographics. \n",
        "\n",
        "## Introduction\n",
        "\n",
        "It is important to ensure data privacy while publicly sharing information that contains sensitive information. There are numerous ways to tackle it and in this post we will use neural networks to generate synthetic data whose statistical features match the actual data. \n",
        "\n",
        "We would be working with the Synthea dataset which is publicly available. Using the patients data from this dataset, we will try to generate synthetic data. \n",
        "\n",
        "https://synthetichealth.github.io/synthea/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnyxMOy5rFMr"
      },
      "source": [
        "#!rm model/*"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ov6AGaUJlrDA"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ewniLY-cBH1"
      },
      "source": [
        "Firstly, download the publicly available adult dataset and unzip it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Njke8p_s9ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dec14c8-7c09-4519-fb4d-e2513d8c288f"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "tf.__version__\n",
        "\n",
        "# Source: https://fairmlbook.org/code/adult.html\n",
        "\n",
        "features = [\"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n",
        "        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n",
        "        \"Hours per week\", \"Country\", \"Target\"] \n",
        "\n",
        "# Change these to local file if available\n",
        "train_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "test_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
        "\n",
        "# This will download 3.8M\n",
        "original_train = pd.read_csv(train_url, names=features, sep=r'\\s*,\\s*', \n",
        "                             engine='python', na_values=\"?\")\n",
        "# This will download 1.9M\n",
        "original_test = pd.read_csv(test_url, names=features, sep=r'\\s*,\\s*', \n",
        "                            engine='python', na_values=\"?\", skiprows=1)\n",
        "\n",
        "original_train = original_train.dropna()\n",
        "original_test = original_test.dropna()\n",
        "\n",
        "original = pd.concat([original_test, original_train])\n",
        "\n",
        "original['Target'] = original['Target'].replace('<=50K', '<=50K').replace('>50K', '>50K')\n",
        "original['Target'] = original['Target'].replace('<=50K.', '<=50K').replace('>50K.', '>50K')\n",
        "\n",
        "original.name = 'AdultDataCombined'\n",
        "original.csv_path = 'AdultDataCombined.csv'\n",
        "\n",
        "print(original)\n",
        "\n",
        "original.to_csv(original.csv_path)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Age     Workclass  fnlwgt  ... Hours per week        Country Target\n",
            "0       25       Private  226802  ...             40  United-States  <=50K\n",
            "1       38       Private   89814  ...             50  United-States  <=50K\n",
            "2       28     Local-gov  336951  ...             40  United-States   >50K\n",
            "3       44       Private  160323  ...             40  United-States   >50K\n",
            "5       34       Private  198693  ...             30  United-States  <=50K\n",
            "...    ...           ...     ...  ...            ...            ...    ...\n",
            "32556   27       Private  257302  ...             38  United-States  <=50K\n",
            "32557   40       Private  154374  ...             40  United-States   >50K\n",
            "32558   58       Private  151910  ...             40  United-States  <=50K\n",
            "32559   22       Private  201490  ...             20  United-States  <=50K\n",
            "32560   52  Self-emp-inc  287927  ...             40  United-States   >50K\n",
            "\n",
            "[45222 rows x 15 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSGJ9yaPePy1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "020bf408-c610-470b-843e-943b9b1fcb85"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('AdultDataCombined.csv')\n",
        "df.drop(['Unnamed: 0','fnlwgt','Education-Num'], axis=1, inplace=True)\n",
        "print(df.columns)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Age', 'Workclass', 'Education', 'Martial Status', 'Occupation',\n",
            "       'Relationship', 'Race', 'Sex', 'Capital Gain', 'Capital Loss',\n",
            "       'Hours per week', 'Country', 'Target'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrWRmZXIlxKe"
      },
      "source": [
        "### Remove unnecessary columns and encode all data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsE5HOw-t3Pd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d93abec2-0670-452d-dcf7-77ddc33374ca"
      },
      "source": [
        "df = original\n",
        "#df.drop(['Unnamed: 0','fnlwgt','Education-Num'], axis=1, inplace=True)\n",
        "print(df.columns)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Age', 'Workclass', 'fnlwgt', 'Education', 'Education-Num',\n",
            "       'Martial Status', 'Occupation', 'Relationship', 'Race', 'Sex',\n",
            "       'Capital Gain', 'Capital Loss', 'Hours per week', 'Country', 'Target'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZlkUlmKcFoI"
      },
      "source": [
        "Next, read patients data and remove fields such as id, date, SSN, name etc. Note, that we are trying to generate synthetic data which can be used to train our deep learning models for some other tasks. For such a model, we don't require fields like id, date, SSN etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Swmxk1NUUcB3"
      },
      "source": [
        "# data configuration\n",
        "file_name = \"AdultDataCombined.csv\"\n",
        "columns_to_drop = ['fnlwgt','Education-Num']\n",
        "columns_to_drop = ['Unnamed: 0','fnlwgt','Education-Num']\n",
        "\n",
        "categorical_features = ['Workclass', 'Education', 'Martial Status', 'Occupation', 'Country','Relationship', 'Race', 'Sex', 'Target']\n",
        "continuous_features = ['Age','Capital Gain', 'Capital Loss', 'Hours per week']\n",
        "col1, col2 = 'Country', 'Race'\n",
        "col_group_by = 'Country'\n",
        "\n",
        "# training configuration\n",
        "noise_dim = 32\n",
        "dim = 128\n",
        "batch_size = 32\n",
        "\n",
        "log_step = 100\n",
        "epochs = 500+1\n",
        "learning_rate = 5e-4\n",
        "models_dir = 'model'"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXUJk9VZARx1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "113e86dc-2c49-4337-9d60-48b0cae8906f"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(file_name)\n",
        "df.drop(columns_to_drop, axis=1, inplace=True)\n",
        "print(df.columns)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Age', 'Workclass', 'Education', 'Martial Status', 'Occupation',\n",
            "       'Relationship', 'Race', 'Sex', 'Capital Gain', 'Capital Loss',\n",
            "       'Hours per week', 'Country', 'Target'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CivI1TYWCEQ-"
      },
      "source": [
        "Next, we will encode all [categorical features](https://en.wikipedia.org/wiki/Categorical_variable) to integer values. We are simply encoding the features to numerical values and are not using one hot encoding as its not required for GANs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd1-a8q9s1f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "c53bfcd5-75a9-4dad-a12c-8d59c3b9474a"
      },
      "source": [
        "for column in categorical_features:\n",
        "  df[column] = df[column].astype('category').cat.codes\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Workclass</th>\n",
              "      <th>Education</th>\n",
              "      <th>Martial Status</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Relationship</th>\n",
              "      <th>Race</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Capital Gain</th>\n",
              "      <th>Capital Loss</th>\n",
              "      <th>Hours per week</th>\n",
              "      <th>Country</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>38</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>44</td>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>7688</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Age  Workclass  Education  ...  Hours per week  Country  Target\n",
              "0   25          2          1  ...              40       38       0\n",
              "1   38          2         11  ...              50       38       0\n",
              "2   28          1          7  ...              40       38       1\n",
              "3   44          2         15  ...              40       38       1\n",
              "4   34          2          0  ...              30       38       0\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZIjhIn1cUAT"
      },
      "source": [
        "Next, we will encode all [continious features](https://en.wikipedia.org/wiki/Continuous_or_discrete_variable?oldformat=true) to equally sized bins. First, lets find the minimum and maximum values for `HEALTHCARE_EXPENSES` and `HEALTHCARE_COVERAGE` and then create bins based on these values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDBp1d-AclvG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "106f6eec-c54d-4615-9948-ab873a0a0600"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "for column in continuous_features:\n",
        "  min = df[column].min()\n",
        "  max = df[column].max()\n",
        "  feature_bins = pd.cut(df[column], bins=21, labels=False)\n",
        "  print(feature_bins)\n",
        "  df.drop([column], axis=1, inplace=True)\n",
        "  df = pd.concat([df, feature_bins], axis=1)\n",
        "df"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0         2\n",
            "1         6\n",
            "2         3\n",
            "3         7\n",
            "4         4\n",
            "         ..\n",
            "45217     2\n",
            "45218     6\n",
            "45219    11\n",
            "45220     1\n",
            "45221    10\n",
            "Name: Age, Length: 45222, dtype: int64\n",
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        1\n",
            "4        0\n",
            "        ..\n",
            "45217    0\n",
            "45218    0\n",
            "45219    0\n",
            "45220    0\n",
            "45221    3\n",
            "Name: Capital Gain, Length: 45222, dtype: int64\n",
            "0        0\n",
            "1        0\n",
            "2        0\n",
            "3        0\n",
            "4        0\n",
            "        ..\n",
            "45217    0\n",
            "45218    0\n",
            "45219    0\n",
            "45220    0\n",
            "45221    0\n",
            "Name: Capital Loss, Length: 45222, dtype: int64\n",
            "0         8\n",
            "1        10\n",
            "2         8\n",
            "3         8\n",
            "4         6\n",
            "         ..\n",
            "45217     7\n",
            "45218     8\n",
            "45219     8\n",
            "45220     4\n",
            "45221     8\n",
            "Name: Hours per week, Length: 45222, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Workclass</th>\n",
              "      <th>Education</th>\n",
              "      <th>Martial Status</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Relationship</th>\n",
              "      <th>Race</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Country</th>\n",
              "      <th>Target</th>\n",
              "      <th>Age</th>\n",
              "      <th>Capital Gain</th>\n",
              "      <th>Capital Loss</th>\n",
              "      <th>Hours per week</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45217</th>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45218</th>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45219</th>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45220</th>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45221</th>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45222 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Workclass  Education  ...  Capital Loss  Hours per week\n",
              "0              2          1  ...             0               8\n",
              "1              2         11  ...             0              10\n",
              "2              1          7  ...             0               8\n",
              "3              2         15  ...             0               8\n",
              "4              2          0  ...             0               6\n",
              "...          ...        ...  ...           ...             ...\n",
              "45217          2          7  ...             0               7\n",
              "45218          2         11  ...             0               8\n",
              "45219          2         11  ...             0               8\n",
              "45220          2         11  ...             0               4\n",
              "45221          3         11  ...             0               8\n",
              "\n",
              "[45222 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eij1RAFCEsss"
      },
      "source": [
        "### Transform the data\n",
        "\n",
        "Next, we apply `PowerTransformer` on all the fields to get a Gaussian distribution for the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzaGA0whtrxz"
      },
      "source": [
        "# from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "\n",
        "# df[df.columns] = PowerTransformer(method='yeo-johnson', standardize=True, copy=True).fit_transform(df[df.columns])\n",
        "\n",
        "# print(df)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_ik_nL2JbFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92ff8501-12b1-4b56-8dba-13f66944537f"
      },
      "source": [
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "\n",
        "pw= PowerTransformer(method='yeo-johnson', standardize=True, copy=True)\n",
        "pwt=pw.fit_transform(df[df.columns])\n",
        "\n",
        "print(df)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       Workclass  Education  ...  Capital Loss  Hours per week\n",
            "0              2          1  ...             0               8\n",
            "1              2         11  ...             0              10\n",
            "2              1          7  ...             0               8\n",
            "3              2         15  ...             0               8\n",
            "4              2          0  ...             0               6\n",
            "...          ...        ...  ...           ...             ...\n",
            "45217          2          7  ...             0               7\n",
            "45218          2         11  ...             0               8\n",
            "45219          2         11  ...             0               8\n",
            "45220          2         11  ...             0               4\n",
            "45221          3         11  ...             0               8\n",
            "\n",
            "[45222 rows x 13 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQjdAY4YLVQK"
      },
      "source": [
        "df[df.columns]=pwt\n",
        "\n",
        "name = 'Target'\n",
        "TARGET_COL_INDEX = 8\n",
        "GENDER_COL_INDEX = 6"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9W-8XDXLhnz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "outputId": "94d75ace-5bff-4559-bc2e-471c5235472f"
      },
      "source": [
        "df"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Workclass</th>\n",
              "      <th>Education</th>\n",
              "      <th>Martial Status</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Relationship</th>\n",
              "      <th>Race</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Country</th>\n",
              "      <th>Target</th>\n",
              "      <th>Age</th>\n",
              "      <th>Capital Gain</th>\n",
              "      <th>Capital Loss</th>\n",
              "      <th>Hours per week</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.135874</td>\n",
              "      <td>-2.087375</td>\n",
              "      <td>0.940015</td>\n",
              "      <td>0.129222</td>\n",
              "      <td>1.115194</td>\n",
              "      <td>-2.491555</td>\n",
              "      <td>0.693813</td>\n",
              "      <td>0.296563</td>\n",
              "      <td>-0.574031</td>\n",
              "      <td>-0.979926</td>\n",
              "      <td>-0.237261</td>\n",
              "      <td>-0.222819</td>\n",
              "      <td>-0.060155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.135874</td>\n",
              "      <td>0.088504</td>\n",
              "      <td>-0.365603</td>\n",
              "      <td>-0.366076</td>\n",
              "      <td>-1.082369</td>\n",
              "      <td>0.402923</td>\n",
              "      <td>0.693813</td>\n",
              "      <td>0.296563</td>\n",
              "      <td>-0.574031</td>\n",
              "      <td>0.213442</td>\n",
              "      <td>-0.237261</td>\n",
              "      <td>-0.222819</td>\n",
              "      <td>0.739046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-1.373407</td>\n",
              "      <td>-1.000172</td>\n",
              "      <td>-0.365603</td>\n",
              "      <td>0.982199</td>\n",
              "      <td>-1.082369</td>\n",
              "      <td>0.402923</td>\n",
              "      <td>0.693813</td>\n",
              "      <td>0.296563</td>\n",
              "      <td>1.742067</td>\n",
              "      <td>-0.631727</td>\n",
              "      <td>-0.237261</td>\n",
              "      <td>-0.222819</td>\n",
              "      <td>-0.060155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.135874</td>\n",
              "      <td>1.406773</td>\n",
              "      <td>-0.365603</td>\n",
              "      <td>0.129222</td>\n",
              "      <td>-1.082369</td>\n",
              "      <td>-2.491555</td>\n",
              "      <td>0.693813</td>\n",
              "      <td>0.296563</td>\n",
              "      <td>1.742067</td>\n",
              "      <td>0.453851</td>\n",
              "      <td>4.214746</td>\n",
              "      <td>-0.222819</td>\n",
              "      <td>-0.060155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.135874</td>\n",
              "      <td>-2.180048</td>\n",
              "      <td>0.940015</td>\n",
              "      <td>0.356604</td>\n",
              "      <td>0.170825</td>\n",
              "      <td>0.402923</td>\n",
              "      <td>0.693813</td>\n",
              "      <td>0.296563</td>\n",
              "      <td>-0.574031</td>\n",
              "      <td>-0.323670</td>\n",
              "      <td>-0.237261</td>\n",
              "      <td>-0.222819</td>\n",
              "      <td>-0.851784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45217</th>\n",
              "      <td>-0.135874</td>\n",
              "      <td>-1.000172</td>\n",
              "      <td>-0.365603</td>\n",
              "      <td>1.363811</td>\n",
              "      <td>1.555688</td>\n",
              "      <td>0.402923</td>\n",
              "      <td>-1.441310</td>\n",
              "      <td>0.296563</td>\n",
              "      <td>-0.574031</td>\n",
              "      <td>-0.979926</td>\n",
              "      <td>-0.237261</td>\n",
              "      <td>-0.222819</td>\n",
              "      <td>-0.457023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45218</th>\n",
              "      <td>-0.135874</td>\n",
              "      <td>0.088504</td>\n",
              "      <td>-0.365603</td>\n",
              "      <td>0.129222</td>\n",
              "      <td>-1.082369</td>\n",
              "      <td>0.402923</td>\n",
              "      <td>0.693813</td>\n",
              "      <td>0.296563</td>\n",
              "      <td>1.742067</td>\n",
              "      <td>0.213442</td>\n",
              "      <td>-0.237261</td>\n",
              "      <td>-0.222819</td>\n",
              "      <td>-0.060155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45219</th>\n",
              "      <td>-0.135874</td>\n",
              "      <td>0.088504</td>\n",
              "      <td>2.191223</td>\n",
              "      <td>-1.706791</td>\n",
              "      <td>1.366606</td>\n",
              "      <td>0.402923</td>\n",
              "      <td>-1.441310</td>\n",
              "      <td>0.296563</td>\n",
              "      <td>-0.574031</td>\n",
              "      <td>1.294236</td>\n",
              "      <td>-0.237261</td>\n",
              "      <td>-0.222819</td>\n",
              "      <td>-0.060155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45220</th>\n",
              "      <td>-0.135874</td>\n",
              "      <td>0.088504</td>\n",
              "      <td>0.940015</td>\n",
              "      <td>-1.706791</td>\n",
              "      <td>1.115194</td>\n",
              "      <td>0.402923</td>\n",
              "      <td>0.693813</td>\n",
              "      <td>0.296563</td>\n",
              "      <td>-0.574031</td>\n",
              "      <td>-1.390624</td>\n",
              "      <td>-0.237261</td>\n",
              "      <td>-0.222819</td>\n",
              "      <td>-1.633730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45221</th>\n",
              "      <td>0.877843</td>\n",
              "      <td>0.088504</td>\n",
              "      <td>-0.365603</td>\n",
              "      <td>-0.641454</td>\n",
              "      <td>1.555688</td>\n",
              "      <td>0.402923</td>\n",
              "      <td>-1.441310</td>\n",
              "      <td>0.296563</td>\n",
              "      <td>1.742067</td>\n",
              "      <td>1.098775</td>\n",
              "      <td>4.214807</td>\n",
              "      <td>-0.222819</td>\n",
              "      <td>-0.060155</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>45222 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Workclass  Education  ...  Capital Loss  Hours per week\n",
              "0      -0.135874  -2.087375  ...     -0.222819       -0.060155\n",
              "1      -0.135874   0.088504  ...     -0.222819        0.739046\n",
              "2      -1.373407  -1.000172  ...     -0.222819       -0.060155\n",
              "3      -0.135874   1.406773  ...     -0.222819       -0.060155\n",
              "4      -0.135874  -2.180048  ...     -0.222819       -0.851784\n",
              "...          ...        ...  ...           ...             ...\n",
              "45217  -0.135874  -1.000172  ...     -0.222819       -0.457023\n",
              "45218  -0.135874   0.088504  ...     -0.222819       -0.060155\n",
              "45219  -0.135874   0.088504  ...     -0.222819       -0.060155\n",
              "45220  -0.135874   0.088504  ...     -0.222819       -1.633730\n",
              "45221   0.877843   0.088504  ...     -0.222819       -0.060155\n",
              "\n",
              "[45222 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvNrKZPdFJJ4"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "Next, lets define the neural network for generating synthetic data. We will be using a [GAN](https://www.wikiwand.com/en/Generative_adversarial_network) network that comprises of an generator and discriminator that tries to beat each other and in the process learns the vector embedding for the data. \n",
        "\n",
        "The model was taken from a [Github repository](https://github.com/ydataai/gan-playground) where it is used to generate synthetic data on credit card fraud data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM7W24g2v7b3"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Concatenate\n",
        "from tensorflow.keras import Model\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "class GAN():\n",
        "    \n",
        "    def __init__(self, gan_args):\n",
        "        [self.batch_size, lr, self.noise_dim,\n",
        "         self.data_dim, layers_dim] = gan_args\n",
        "        print(1)\n",
        "\n",
        "        self.generator = Generator(self.batch_size).\\\n",
        "            build_model(input_shape=(self.noise_dim,), dim=layers_dim, data_dim=self.data_dim)\n",
        "        print(2)\n",
        "\n",
        "        self.discriminator = Discriminator(self.batch_size).\\\n",
        "            build_model(input_shape=(self.data_dim,), dim=layers_dim)\n",
        "        print(3)\n",
        "\n",
        "        optimizer = Adam(lr, 0.5)\n",
        "\n",
        "        # Build and compile the discriminator\n",
        "        self.discriminator.compile(loss='binary_crossentropy',\n",
        "                                   optimizer=optimizer,\n",
        "                                   metrics=['accuracy'])\n",
        "        print(4)\n",
        "\n",
        "        # The generator takes noise as input and generates imgs\n",
        "        z = Input(shape=(self.noise_dim,))\n",
        "        y = Input(shape=(1,))\n",
        "        s = Input(shape=(1,))\n",
        "        record = self.generator([z, y, s])\n",
        "\n",
        "        print(5)\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "        print(6)\n",
        "\n",
        "        # The discriminator takes generated images as input and determines validity\n",
        "        validity = self.discriminator(record)\n",
        "        print(7)\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator)\n",
        "        # Trains the generator to fool the discriminator\n",
        "        self.combined = Model([z,y,s], validity)\n",
        "        print(8)\n",
        "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "        print(9)\n",
        "    def get_data_batch(self, train, batch_size, seed=0):\n",
        "        # # random sampling - some samples will have excessively low or high sampling, but easy to implement\n",
        "        # np.random.seed(seed)\n",
        "        # x = train.loc[ np.random.choice(train.index, batch_size) ].values\n",
        "        # iterate through shuffled indices, so every sample gets covered evenly\n",
        "\n",
        "        start_i = (batch_size * seed) % len(train)\n",
        "        stop_i = start_i + batch_size\n",
        "        shuffle_seed = (batch_size * seed) // len(train)\n",
        "        np.random.seed(shuffle_seed)\n",
        "        train_ix = np.random.choice(list(train.index), replace=False, size=len(train))  # wasteful to shuffle every time\n",
        "        train_ix = list(train_ix) + list(train_ix)  # duplicate to cover ranges past the end of the set\n",
        "        x = train.loc[train_ix[start_i: stop_i]].values\n",
        "        return np.reshape(x, (batch_size, -1))\n",
        "        \n",
        "    def train(self, data, train_arguments):\n",
        "        [cache_prefix, epochs, sample_interval] = train_arguments\n",
        "        \n",
        "        data_cols = data.columns\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((self.batch_size, 1))\n",
        "        fake = np.zeros((self.batch_size, 1))\n",
        "\n",
        "        for epoch in range(epochs):    \n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "            batch_data = self.get_data_batch(data, self.batch_size)\n",
        "            #print(batch_data)\n",
        "            # TODO: fix bad code\n",
        "            labelsTarget = batch_data[:,TARGET_COL_INDEX]\n",
        "            labelsGender = batch_data[:,GENDER_COL_INDEX]\n",
        "            #print(\"labels created\")\n",
        "\n",
        "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
        "            # Generate a batch of new images\n",
        "            gen_data = self.generator.predict([noise, labelsTarget, labelsGender])\n",
        "            #print(\"Noise and gen_data\")\n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch(batch_data, valid)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_data, fake)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "    \n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "            noise = tf.random.normal((self.batch_size, self.noise_dim))\n",
        "            # Train the generator (to have the discriminator label samples as valid)\n",
        "            g_loss = self.combined.train_on_batch([noise, labelsTarget, labelsGender], valid)\n",
        "    \n",
        "            # Plot the progress\n",
        "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss))\n",
        "    \n",
        "            # If at save interval => save generated events\n",
        "            if epoch % sample_interval == 0:\n",
        "                #Test here data generation step\n",
        "                # save model checkpoints\n",
        "                model_checkpoint_base_name = 'model/' + cache_prefix + '_{}_model_weights_step_{}.h5'\n",
        "                self.generator.save_weights(model_checkpoint_base_name.format('generator', epoch))\n",
        "                self.discriminator.save_weights(model_checkpoint_base_name.format('discriminator', epoch))\n",
        "\n",
        "                #Here is generating the data\n",
        "                z = tf.random.normal((432, self.noise_dim))\n",
        "                gen_data = self.generator([z, np.zeros((432, )), np.zeros((432, ))])\n",
        "                print('generated_data')\n",
        "\n",
        "    def save(self, path, name):\n",
        "        assert os.path.isdir(path) == True, \\\n",
        "            \"Please provide a valid path. Path must be a directory.\"\n",
        "        model_path = os.path.join(path, name)\n",
        "        self.generator.save_weights(model_path)  # Load the generator\n",
        "        return\n",
        "    \n",
        "    def load(self, path):\n",
        "        assert os.path.isdir(path) == True, \\\n",
        "            \"Please provide a valid path. Path must be a directory.\"\n",
        "        self.generator = Generator(self.batch_size)\n",
        "        self.generator = self.generator.load_weights(path)\n",
        "        return self.generator\n",
        "    \n",
        "class Generator():\n",
        "    def __init__(self, batch_size):\n",
        "        self.batch_size=batch_size\n",
        "        \n",
        "    def build_model(self, input_shape, dim, data_dim):\n",
        "        input= Input(shape=input_shape, batch_size=self.batch_size)\n",
        "        inputIncome= Input(shape=(1,), batch_size=self.batch_size)\n",
        "        inputGender= Input(shape=(1,), batch_size=self.batch_size)\n",
        "        t = Dense(dim, activation='relu')(input)\n",
        "        y = Dense(dim, activation='relu')(inputIncome)\n",
        "        s = Dense(dim, activation='relu')(inputGender)\n",
        "        x = Concatenate(axis=1)([t, y, s])\n",
        "        x = Dense(dim * 2, activation='relu')(x)\n",
        "        x = Dense(dim * 4, activation='relu')(x)\n",
        "        x = Dense(data_dim)(x)\n",
        "        return Model(inputs=[input,inputIncome,inputGender], outputs=x)\n",
        "\n",
        "class Discriminator():\n",
        "    def __init__(self,batch_size):\n",
        "        self.batch_size=batch_size\n",
        "    \n",
        "    def build_model(self, input_shape, dim):\n",
        "        input = Input(shape=input_shape, batch_size=self.batch_size)\n",
        "        x = Dense(dim * 4, activation='relu')(input)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Dense(dim * 2, activation='relu')(x)\n",
        "        x = Dropout(0.1)(x)\n",
        "        x = Dense(dim, activation='relu')(x)\n",
        "        x = Dense(1, activation='sigmoid')(x)\n",
        "        return Model(inputs=input, outputs=x)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06PBJKvMGUdQ"
      },
      "source": [
        "Next, lets define the training parameters for the GAN network. We would be using a batch size of 32 and train it for 5000 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCpmysfBz_Eo"
      },
      "source": [
        "data_cols = df.columns"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDmbScY6vRCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c15e604-c1c8-4bfd-cda2-66cbd033fc02"
      },
      "source": [
        "#Define the GAN and training parameters\n",
        "df[data_cols] = df[data_cols]\n",
        "\n",
        "print(df.shape[1])\n",
        "\n",
        "gan_args = [batch_size, learning_rate, noise_dim, df.shape[1], dim]\n",
        "train_args = ['', epochs, log_step]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUQDsxJo3rL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bffdaa29-590c-45bc-8ddf-565395ab0485"
      },
      "source": [
        "!mkdir model\n",
        "!mkdir model/gan\n",
        "!mkdir model/gan/saved"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘model’: File exists\n",
            "mkdir: cannot create directory ‘model/gan’: File exists\n",
            "mkdir: cannot create directory ‘model/gan/saved’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_U40WJaGfUd"
      },
      "source": [
        "Finally, let's run the training and see if the model is able to learn something. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqWKRukawP41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b76484c-65e7-4a7c-d21e-cafdc16ca9e1"
      },
      "source": [
        "model = GAN\n",
        "\n",
        "#Training the GAN model chosen: Vanilla GAN, CGAN, DCGAN, etc.\n",
        "synthesizer = model(gan_args)\n",
        "print(synthesizer)\n",
        "synthesizer.train(df, train_args)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "<__main__.GAN object at 0x7fa9338ae690>\n",
            "0 [D loss: 0.710547, acc.: 14.06%] [G loss: 0.678809]\n",
            "generated_data\n",
            "1 [D loss: 0.660025, acc.: 50.00%] [G loss: 0.657789]\n",
            "2 [D loss: 0.636781, acc.: 50.00%] [G loss: 0.635769]\n",
            "3 [D loss: 0.618122, acc.: 50.00%] [G loss: 0.625309]\n",
            "4 [D loss: 0.594617, acc.: 50.00%] [G loss: 0.672618]\n",
            "5 [D loss: 0.543660, acc.: 51.56%] [G loss: 0.780434]\n",
            "6 [D loss: 0.494182, acc.: 78.12%] [G loss: 0.881012]\n",
            "7 [D loss: 0.466954, acc.: 93.75%] [G loss: 0.963668]\n",
            "8 [D loss: 0.494827, acc.: 70.31%] [G loss: 0.907681]\n",
            "9 [D loss: 0.538780, acc.: 48.44%] [G loss: 0.856179]\n",
            "10 [D loss: 0.592288, acc.: 45.31%] [G loss: 0.821922]\n",
            "11 [D loss: 0.603879, acc.: 46.88%] [G loss: 0.841693]\n",
            "12 [D loss: 0.579142, acc.: 50.00%] [G loss: 0.887546]\n",
            "13 [D loss: 0.604614, acc.: 43.75%] [G loss: 0.910955]\n",
            "14 [D loss: 0.556718, acc.: 53.12%] [G loss: 0.982512]\n",
            "15 [D loss: 0.542458, acc.: 70.31%] [G loss: 1.053800]\n",
            "16 [D loss: 0.498415, acc.: 87.50%] [G loss: 1.211765]\n",
            "17 [D loss: 0.519577, acc.: 76.56%] [G loss: 1.172651]\n",
            "18 [D loss: 0.527290, acc.: 70.31%] [G loss: 1.110987]\n",
            "19 [D loss: 0.543243, acc.: 68.75%] [G loss: 1.073687]\n",
            "20 [D loss: 0.506338, acc.: 81.25%] [G loss: 1.147689]\n",
            "21 [D loss: 0.447572, acc.: 95.31%] [G loss: 1.281264]\n",
            "22 [D loss: 0.417673, acc.: 93.75%] [G loss: 1.198051]\n",
            "23 [D loss: 0.432877, acc.: 95.31%] [G loss: 1.118064]\n",
            "24 [D loss: 0.454550, acc.: 87.50%] [G loss: 1.006913]\n",
            "25 [D loss: 0.461533, acc.: 73.44%] [G loss: 0.943686]\n",
            "26 [D loss: 0.473539, acc.: 70.31%] [G loss: 0.952117]\n",
            "27 [D loss: 0.505817, acc.: 57.81%] [G loss: 1.020303]\n",
            "28 [D loss: 0.459118, acc.: 81.25%] [G loss: 1.097127]\n",
            "29 [D loss: 0.443093, acc.: 89.06%] [G loss: 1.231106]\n",
            "30 [D loss: 0.401212, acc.: 92.19%] [G loss: 1.330099]\n",
            "31 [D loss: 0.368633, acc.: 95.31%] [G loss: 1.380459]\n",
            "32 [D loss: 0.340684, acc.: 95.31%] [G loss: 1.377792]\n",
            "33 [D loss: 0.403506, acc.: 92.19%] [G loss: 1.279434]\n",
            "34 [D loss: 0.417728, acc.: 84.38%] [G loss: 1.099854]\n",
            "35 [D loss: 0.442464, acc.: 76.56%] [G loss: 1.010485]\n",
            "36 [D loss: 0.469814, acc.: 70.31%] [G loss: 1.099831]\n",
            "37 [D loss: 0.416706, acc.: 82.81%] [G loss: 1.260734]\n",
            "38 [D loss: 0.367444, acc.: 92.19%] [G loss: 1.397595]\n",
            "39 [D loss: 0.431507, acc.: 85.94%] [G loss: 1.362065]\n",
            "40 [D loss: 0.400582, acc.: 84.38%] [G loss: 1.453546]\n",
            "41 [D loss: 0.378550, acc.: 95.31%] [G loss: 1.566609]\n",
            "42 [D loss: 0.443249, acc.: 89.06%] [G loss: 1.413648]\n",
            "43 [D loss: 0.421432, acc.: 85.94%] [G loss: 1.530240]\n",
            "44 [D loss: 0.402303, acc.: 89.06%] [G loss: 1.564644]\n",
            "45 [D loss: 0.315110, acc.: 92.19%] [G loss: 1.525325]\n",
            "46 [D loss: 0.303155, acc.: 95.31%] [G loss: 1.585481]\n",
            "47 [D loss: 0.269289, acc.: 98.44%] [G loss: 1.607005]\n",
            "48 [D loss: 0.260268, acc.: 98.44%] [G loss: 1.630649]\n",
            "49 [D loss: 0.255057, acc.: 95.31%] [G loss: 1.659864]\n",
            "50 [D loss: 0.304331, acc.: 93.75%] [G loss: 1.750008]\n",
            "51 [D loss: 0.276215, acc.: 96.88%] [G loss: 1.761449]\n",
            "52 [D loss: 0.225068, acc.: 96.88%] [G loss: 1.853971]\n",
            "53 [D loss: 0.254572, acc.: 93.75%] [G loss: 1.949142]\n",
            "54 [D loss: 0.196387, acc.: 98.44%] [G loss: 1.972306]\n",
            "55 [D loss: 0.213985, acc.: 98.44%] [G loss: 1.924451]\n",
            "56 [D loss: 0.257811, acc.: 98.44%] [G loss: 1.941048]\n",
            "57 [D loss: 0.324846, acc.: 95.31%] [G loss: 1.933612]\n",
            "58 [D loss: 0.427070, acc.: 82.81%] [G loss: 2.148847]\n",
            "59 [D loss: 0.385706, acc.: 85.94%] [G loss: 2.241633]\n",
            "60 [D loss: 0.411220, acc.: 87.50%] [G loss: 2.475236]\n",
            "61 [D loss: 0.495251, acc.: 79.69%] [G loss: 2.614695]\n",
            "62 [D loss: 0.539636, acc.: 75.00%] [G loss: 3.002945]\n",
            "63 [D loss: 0.371795, acc.: 84.38%] [G loss: 2.778666]\n",
            "64 [D loss: 0.404604, acc.: 85.94%] [G loss: 2.925306]\n",
            "65 [D loss: 0.274372, acc.: 92.19%] [G loss: 2.925810]\n",
            "66 [D loss: 0.329192, acc.: 93.75%] [G loss: 2.985536]\n",
            "67 [D loss: 0.374394, acc.: 92.19%] [G loss: 2.977712]\n",
            "68 [D loss: 0.349216, acc.: 89.06%] [G loss: 2.828419]\n",
            "69 [D loss: 0.515026, acc.: 71.88%] [G loss: 3.545818]\n",
            "70 [D loss: 0.325900, acc.: 85.94%] [G loss: 2.905406]\n",
            "71 [D loss: 0.387273, acc.: 85.94%] [G loss: 2.711191]\n",
            "72 [D loss: 0.308952, acc.: 87.50%] [G loss: 2.553401]\n",
            "73 [D loss: 0.483421, acc.: 76.56%] [G loss: 2.913111]\n",
            "74 [D loss: 0.361415, acc.: 87.50%] [G loss: 2.870760]\n",
            "75 [D loss: 0.336403, acc.: 89.06%] [G loss: 2.836084]\n",
            "76 [D loss: 0.274259, acc.: 93.75%] [G loss: 2.933169]\n",
            "77 [D loss: 0.180204, acc.: 95.31%] [G loss: 3.088756]\n",
            "78 [D loss: 0.263709, acc.: 95.31%] [G loss: 3.018261]\n",
            "79 [D loss: 0.289594, acc.: 92.19%] [G loss: 3.163221]\n",
            "80 [D loss: 0.443750, acc.: 78.12%] [G loss: 3.168899]\n",
            "81 [D loss: 0.315800, acc.: 87.50%] [G loss: 3.249913]\n",
            "82 [D loss: 0.358682, acc.: 89.06%] [G loss: 3.469987]\n",
            "83 [D loss: 0.340445, acc.: 89.06%] [G loss: 3.212264]\n",
            "84 [D loss: 0.377579, acc.: 92.19%] [G loss: 2.872395]\n",
            "85 [D loss: 0.407036, acc.: 76.56%] [G loss: 3.400702]\n",
            "86 [D loss: 0.316541, acc.: 92.19%] [G loss: 2.859594]\n",
            "87 [D loss: 0.560270, acc.: 79.69%] [G loss: 3.006874]\n",
            "88 [D loss: 0.383292, acc.: 87.50%] [G loss: 3.239214]\n",
            "89 [D loss: 0.329434, acc.: 89.06%] [G loss: 3.227017]\n",
            "90 [D loss: 0.328513, acc.: 87.50%] [G loss: 3.337288]\n",
            "91 [D loss: 0.236421, acc.: 93.75%] [G loss: 3.031784]\n",
            "92 [D loss: 0.417184, acc.: 79.69%] [G loss: 3.544950]\n",
            "93 [D loss: 0.212400, acc.: 93.75%] [G loss: 3.299610]\n",
            "94 [D loss: 0.366114, acc.: 89.06%] [G loss: 3.166253]\n",
            "95 [D loss: 0.211151, acc.: 95.31%] [G loss: 3.426481]\n",
            "96 [D loss: 0.269500, acc.: 90.62%] [G loss: 3.613379]\n",
            "97 [D loss: 0.394171, acc.: 78.12%] [G loss: 3.824131]\n",
            "98 [D loss: 0.327923, acc.: 90.62%] [G loss: 2.992580]\n",
            "99 [D loss: 0.761278, acc.: 54.69%] [G loss: 3.356424]\n",
            "100 [D loss: 0.370244, acc.: 92.19%] [G loss: 3.501779]\n",
            "generated_data\n",
            "101 [D loss: 0.457002, acc.: 87.50%] [G loss: 2.972788]\n",
            "102 [D loss: 0.414970, acc.: 87.50%] [G loss: 3.400927]\n",
            "103 [D loss: 0.250753, acc.: 92.19%] [G loss: 3.486167]\n",
            "104 [D loss: 0.245640, acc.: 90.62%] [G loss: 3.382218]\n",
            "105 [D loss: 0.195909, acc.: 92.19%] [G loss: 3.437079]\n",
            "106 [D loss: 0.160652, acc.: 95.31%] [G loss: 3.370407]\n",
            "107 [D loss: 0.180821, acc.: 92.19%] [G loss: 3.236513]\n",
            "108 [D loss: 0.203802, acc.: 95.31%] [G loss: 3.982791]\n",
            "109 [D loss: 0.166564, acc.: 95.31%] [G loss: 3.748328]\n",
            "110 [D loss: 0.246565, acc.: 95.31%] [G loss: 3.367224]\n",
            "111 [D loss: 0.375372, acc.: 84.38%] [G loss: 4.287580]\n",
            "112 [D loss: 0.240405, acc.: 93.75%] [G loss: 3.904537]\n",
            "113 [D loss: 0.394641, acc.: 82.81%] [G loss: 3.897271]\n",
            "114 [D loss: 0.197422, acc.: 92.19%] [G loss: 3.677680]\n",
            "115 [D loss: 0.404007, acc.: 82.81%] [G loss: 3.918614]\n",
            "116 [D loss: 0.226013, acc.: 95.31%] [G loss: 3.580557]\n",
            "117 [D loss: 0.497365, acc.: 78.12%] [G loss: 4.972345]\n",
            "118 [D loss: 0.345088, acc.: 89.06%] [G loss: 4.082532]\n",
            "119 [D loss: 0.689680, acc.: 64.06%] [G loss: 4.842448]\n",
            "120 [D loss: 0.405967, acc.: 87.50%] [G loss: 4.058019]\n",
            "121 [D loss: 0.599938, acc.: 75.00%] [G loss: 4.097586]\n",
            "122 [D loss: 0.369345, acc.: 90.62%] [G loss: 3.832334]\n",
            "123 [D loss: 0.523311, acc.: 71.88%] [G loss: 4.311422]\n",
            "124 [D loss: 0.364926, acc.: 87.50%] [G loss: 3.994769]\n",
            "125 [D loss: 0.704478, acc.: 62.50%] [G loss: 4.306180]\n",
            "126 [D loss: 0.459096, acc.: 87.50%] [G loss: 4.074270]\n",
            "127 [D loss: 0.615870, acc.: 81.25%] [G loss: 3.717232]\n",
            "128 [D loss: 0.465872, acc.: 85.94%] [G loss: 3.673562]\n",
            "129 [D loss: 0.553048, acc.: 78.12%] [G loss: 4.417551]\n",
            "130 [D loss: 0.407276, acc.: 84.38%] [G loss: 4.180356]\n",
            "131 [D loss: 0.534103, acc.: 78.12%] [G loss: 4.466619]\n",
            "132 [D loss: 0.334718, acc.: 85.94%] [G loss: 4.096786]\n",
            "133 [D loss: 0.501683, acc.: 73.44%] [G loss: 4.352093]\n",
            "134 [D loss: 0.407269, acc.: 85.94%] [G loss: 3.826059]\n",
            "135 [D loss: 0.637852, acc.: 59.38%] [G loss: 3.913692]\n",
            "136 [D loss: 0.488207, acc.: 85.94%] [G loss: 3.273824]\n",
            "137 [D loss: 0.633196, acc.: 76.56%] [G loss: 3.203390]\n",
            "138 [D loss: 0.483003, acc.: 82.81%] [G loss: 3.651915]\n",
            "139 [D loss: 0.474464, acc.: 82.81%] [G loss: 3.497153]\n",
            "140 [D loss: 0.457266, acc.: 87.50%] [G loss: 3.271561]\n",
            "141 [D loss: 0.392034, acc.: 85.94%] [G loss: 3.263769]\n",
            "142 [D loss: 0.387966, acc.: 85.94%] [G loss: 3.474681]\n",
            "143 [D loss: 0.207810, acc.: 93.75%] [G loss: 3.108760]\n",
            "144 [D loss: 0.314048, acc.: 90.62%] [G loss: 3.364909]\n",
            "145 [D loss: 0.183793, acc.: 93.75%] [G loss: 3.564885]\n",
            "146 [D loss: 0.242945, acc.: 96.88%] [G loss: 2.894045]\n",
            "147 [D loss: 0.269474, acc.: 93.75%] [G loss: 3.368581]\n",
            "148 [D loss: 0.224415, acc.: 95.31%] [G loss: 3.680798]\n",
            "149 [D loss: 0.281067, acc.: 92.19%] [G loss: 3.654370]\n",
            "150 [D loss: 0.269498, acc.: 92.19%] [G loss: 4.141314]\n",
            "151 [D loss: 0.296137, acc.: 93.75%] [G loss: 3.782814]\n",
            "152 [D loss: 0.474632, acc.: 85.94%] [G loss: 3.640389]\n",
            "153 [D loss: 0.405894, acc.: 89.06%] [G loss: 3.637299]\n",
            "154 [D loss: 0.486241, acc.: 90.62%] [G loss: 3.343245]\n",
            "155 [D loss: 0.415070, acc.: 92.19%] [G loss: 3.527194]\n",
            "156 [D loss: 0.436101, acc.: 87.50%] [G loss: 3.094610]\n",
            "157 [D loss: 0.513356, acc.: 82.81%] [G loss: 3.236699]\n",
            "158 [D loss: 0.525783, acc.: 78.12%] [G loss: 3.444896]\n",
            "159 [D loss: 0.655792, acc.: 68.75%] [G loss: 3.204442]\n",
            "160 [D loss: 0.584876, acc.: 75.00%] [G loss: 3.104247]\n",
            "161 [D loss: 0.714518, acc.: 62.50%] [G loss: 3.704678]\n",
            "162 [D loss: 0.476307, acc.: 81.25%] [G loss: 3.645837]\n",
            "163 [D loss: 0.576762, acc.: 81.25%] [G loss: 3.219183]\n",
            "164 [D loss: 0.547747, acc.: 82.81%] [G loss: 3.220513]\n",
            "165 [D loss: 0.517375, acc.: 84.38%] [G loss: 3.179985]\n",
            "166 [D loss: 0.456206, acc.: 85.94%] [G loss: 2.917919]\n",
            "167 [D loss: 0.422121, acc.: 89.06%] [G loss: 3.177840]\n",
            "168 [D loss: 0.327678, acc.: 89.06%] [G loss: 3.016326]\n",
            "169 [D loss: 0.407614, acc.: 89.06%] [G loss: 3.224345]\n",
            "170 [D loss: 0.268788, acc.: 90.62%] [G loss: 2.975055]\n",
            "171 [D loss: 0.295333, acc.: 89.06%] [G loss: 2.689574]\n",
            "172 [D loss: 0.316830, acc.: 90.62%] [G loss: 2.917829]\n",
            "173 [D loss: 0.311700, acc.: 89.06%] [G loss: 3.195462]\n",
            "174 [D loss: 0.277219, acc.: 90.62%] [G loss: 2.906935]\n",
            "175 [D loss: 0.296340, acc.: 90.62%] [G loss: 2.670227]\n",
            "176 [D loss: 0.248695, acc.: 92.19%] [G loss: 2.878542]\n",
            "177 [D loss: 0.284697, acc.: 93.75%] [G loss: 2.668530]\n",
            "178 [D loss: 0.245454, acc.: 95.31%] [G loss: 3.181414]\n",
            "179 [D loss: 0.201525, acc.: 93.75%] [G loss: 2.946764]\n",
            "180 [D loss: 0.251052, acc.: 92.19%] [G loss: 3.639250]\n",
            "181 [D loss: 0.151479, acc.: 95.31%] [G loss: 3.732496]\n",
            "182 [D loss: 0.158045, acc.: 98.44%] [G loss: 2.821068]\n",
            "183 [D loss: 0.255613, acc.: 95.31%] [G loss: 2.726897]\n",
            "184 [D loss: 0.260052, acc.: 92.19%] [G loss: 2.914761]\n",
            "185 [D loss: 0.257988, acc.: 93.75%] [G loss: 2.941826]\n",
            "186 [D loss: 0.341193, acc.: 90.62%] [G loss: 3.158865]\n",
            "187 [D loss: 0.379716, acc.: 85.94%] [G loss: 3.391798]\n",
            "188 [D loss: 0.401620, acc.: 87.50%] [G loss: 3.705334]\n",
            "189 [D loss: 0.478581, acc.: 76.56%] [G loss: 3.625020]\n",
            "190 [D loss: 0.481651, acc.: 79.69%] [G loss: 3.775602]\n",
            "191 [D loss: 0.441179, acc.: 87.50%] [G loss: 3.404226]\n",
            "192 [D loss: 0.611788, acc.: 68.75%] [G loss: 4.085331]\n",
            "193 [D loss: 0.420270, acc.: 87.50%] [G loss: 3.487007]\n",
            "194 [D loss: 0.538807, acc.: 81.25%] [G loss: 3.384920]\n",
            "195 [D loss: 0.358501, acc.: 87.50%] [G loss: 3.345253]\n",
            "196 [D loss: 0.414238, acc.: 89.06%] [G loss: 3.226094]\n",
            "197 [D loss: 0.444691, acc.: 87.50%] [G loss: 3.163431]\n",
            "198 [D loss: 0.487070, acc.: 85.94%] [G loss: 3.480671]\n",
            "199 [D loss: 0.472602, acc.: 89.06%] [G loss: 3.365643]\n",
            "200 [D loss: 0.530202, acc.: 87.50%] [G loss: 2.907918]\n",
            "generated_data\n",
            "201 [D loss: 0.560524, acc.: 79.69%] [G loss: 3.240304]\n",
            "202 [D loss: 0.496999, acc.: 84.38%] [G loss: 3.335656]\n",
            "203 [D loss: 0.487980, acc.: 85.94%] [G loss: 2.850791]\n",
            "204 [D loss: 0.448579, acc.: 84.38%] [G loss: 3.166363]\n",
            "205 [D loss: 0.432701, acc.: 85.94%] [G loss: 3.185242]\n",
            "206 [D loss: 0.410766, acc.: 87.50%] [G loss: 3.071590]\n",
            "207 [D loss: 0.386978, acc.: 89.06%] [G loss: 3.136330]\n",
            "208 [D loss: 0.466314, acc.: 84.38%] [G loss: 3.495911]\n",
            "209 [D loss: 0.438796, acc.: 82.81%] [G loss: 3.371445]\n",
            "210 [D loss: 0.544165, acc.: 81.25%] [G loss: 3.571811]\n",
            "211 [D loss: 0.486647, acc.: 82.81%] [G loss: 3.113535]\n",
            "212 [D loss: 0.658818, acc.: 68.75%] [G loss: 2.832166]\n",
            "213 [D loss: 0.610488, acc.: 81.25%] [G loss: 2.827481]\n",
            "214 [D loss: 0.596921, acc.: 81.25%] [G loss: 2.697609]\n",
            "215 [D loss: 0.657115, acc.: 75.00%] [G loss: 2.578928]\n",
            "216 [D loss: 0.564778, acc.: 81.25%] [G loss: 2.402389]\n",
            "217 [D loss: 0.617921, acc.: 84.38%] [G loss: 2.517436]\n",
            "218 [D loss: 0.475690, acc.: 85.94%] [G loss: 2.452194]\n",
            "219 [D loss: 0.465155, acc.: 84.38%] [G loss: 2.614715]\n",
            "220 [D loss: 0.398655, acc.: 82.81%] [G loss: 2.439279]\n",
            "221 [D loss: 0.420952, acc.: 79.69%] [G loss: 2.827373]\n",
            "222 [D loss: 0.315192, acc.: 89.06%] [G loss: 2.748012]\n",
            "223 [D loss: 0.509244, acc.: 73.44%] [G loss: 3.125767]\n",
            "224 [D loss: 0.359302, acc.: 84.38%] [G loss: 2.961418]\n",
            "225 [D loss: 0.476141, acc.: 79.69%] [G loss: 2.663337]\n",
            "226 [D loss: 0.412825, acc.: 82.81%] [G loss: 2.760800]\n",
            "227 [D loss: 0.408629, acc.: 82.81%] [G loss: 2.340890]\n",
            "228 [D loss: 0.443073, acc.: 89.06%] [G loss: 2.470062]\n",
            "229 [D loss: 0.476369, acc.: 85.94%] [G loss: 2.480664]\n",
            "230 [D loss: 0.359036, acc.: 85.94%] [G loss: 2.217598]\n",
            "231 [D loss: 0.402221, acc.: 85.94%] [G loss: 2.312122]\n",
            "232 [D loss: 0.301868, acc.: 92.19%] [G loss: 2.359988]\n",
            "233 [D loss: 0.331541, acc.: 93.75%] [G loss: 2.522813]\n",
            "234 [D loss: 0.291118, acc.: 93.75%] [G loss: 2.807835]\n",
            "235 [D loss: 0.268500, acc.: 90.62%] [G loss: 2.858658]\n",
            "236 [D loss: 0.336363, acc.: 90.62%] [G loss: 3.020897]\n",
            "237 [D loss: 0.283599, acc.: 93.75%] [G loss: 3.061774]\n",
            "238 [D loss: 0.308423, acc.: 90.62%] [G loss: 3.162493]\n",
            "239 [D loss: 0.296769, acc.: 90.62%] [G loss: 2.860075]\n",
            "240 [D loss: 0.277451, acc.: 89.06%] [G loss: 2.795478]\n",
            "241 [D loss: 0.337914, acc.: 89.06%] [G loss: 3.078085]\n",
            "242 [D loss: 0.294175, acc.: 87.50%] [G loss: 2.994658]\n",
            "243 [D loss: 0.298819, acc.: 92.19%] [G loss: 2.736016]\n",
            "244 [D loss: 0.274517, acc.: 92.19%] [G loss: 2.725729]\n",
            "245 [D loss: 0.326593, acc.: 90.62%] [G loss: 2.853947]\n",
            "246 [D loss: 0.349282, acc.: 89.06%] [G loss: 2.872684]\n",
            "247 [D loss: 0.272014, acc.: 93.75%] [G loss: 2.501228]\n",
            "248 [D loss: 0.381066, acc.: 84.38%] [G loss: 2.205839]\n",
            "249 [D loss: 0.451068, acc.: 81.25%] [G loss: 2.488620]\n",
            "250 [D loss: 0.401059, acc.: 84.38%] [G loss: 2.586909]\n",
            "251 [D loss: 0.322618, acc.: 90.62%] [G loss: 2.481069]\n",
            "252 [D loss: 0.405118, acc.: 84.38%] [G loss: 2.515807]\n",
            "253 [D loss: 0.275377, acc.: 89.06%] [G loss: 2.743428]\n",
            "254 [D loss: 0.312792, acc.: 89.06%] [G loss: 2.673976]\n",
            "255 [D loss: 0.277534, acc.: 89.06%] [G loss: 2.899411]\n",
            "256 [D loss: 0.250067, acc.: 89.06%] [G loss: 2.799822]\n",
            "257 [D loss: 0.231783, acc.: 89.06%] [G loss: 2.732235]\n",
            "258 [D loss: 0.231736, acc.: 92.19%] [G loss: 2.806802]\n",
            "259 [D loss: 0.215582, acc.: 93.75%] [G loss: 2.942402]\n",
            "260 [D loss: 0.271495, acc.: 90.62%] [G loss: 3.414064]\n",
            "261 [D loss: 0.171054, acc.: 90.62%] [G loss: 3.291339]\n",
            "262 [D loss: 0.278777, acc.: 87.50%] [G loss: 3.146103]\n",
            "263 [D loss: 0.245336, acc.: 90.62%] [G loss: 3.233228]\n",
            "264 [D loss: 0.224900, acc.: 89.06%] [G loss: 3.195089]\n",
            "265 [D loss: 0.305665, acc.: 85.94%] [G loss: 3.402576]\n",
            "266 [D loss: 0.204675, acc.: 89.06%] [G loss: 3.142479]\n",
            "267 [D loss: 0.250106, acc.: 92.19%] [G loss: 3.226441]\n",
            "268 [D loss: 0.216049, acc.: 92.19%] [G loss: 3.111742]\n",
            "269 [D loss: 0.208274, acc.: 95.31%] [G loss: 3.012172]\n",
            "270 [D loss: 0.209915, acc.: 95.31%] [G loss: 2.702387]\n",
            "271 [D loss: 0.143964, acc.: 98.44%] [G loss: 2.894413]\n",
            "272 [D loss: 0.154223, acc.: 96.88%] [G loss: 3.357343]\n",
            "273 [D loss: 0.161962, acc.: 95.31%] [G loss: 3.055550]\n",
            "274 [D loss: 0.288032, acc.: 90.62%] [G loss: 3.671792]\n",
            "275 [D loss: 0.226973, acc.: 92.19%] [G loss: 3.705800]\n",
            "276 [D loss: 0.248564, acc.: 92.19%] [G loss: 3.922933]\n",
            "277 [D loss: 0.223448, acc.: 96.88%] [G loss: 3.807870]\n",
            "278 [D loss: 0.260146, acc.: 92.19%] [G loss: 3.644056]\n",
            "279 [D loss: 0.115539, acc.: 96.88%] [G loss: 3.826172]\n",
            "280 [D loss: 0.229083, acc.: 95.31%] [G loss: 3.899308]\n",
            "281 [D loss: 0.159016, acc.: 95.31%] [G loss: 3.600663]\n",
            "282 [D loss: 0.168484, acc.: 98.44%] [G loss: 3.412883]\n",
            "283 [D loss: 0.161854, acc.: 95.31%] [G loss: 4.278771]\n",
            "284 [D loss: 0.183425, acc.: 95.31%] [G loss: 4.097059]\n",
            "285 [D loss: 0.170128, acc.: 95.31%] [G loss: 3.924021]\n",
            "286 [D loss: 0.165807, acc.: 95.31%] [G loss: 3.658002]\n",
            "287 [D loss: 0.245288, acc.: 90.62%] [G loss: 4.127912]\n",
            "288 [D loss: 0.139971, acc.: 93.75%] [G loss: 4.301779]\n",
            "289 [D loss: 0.147366, acc.: 95.31%] [G loss: 3.727058]\n",
            "290 [D loss: 0.230309, acc.: 93.75%] [G loss: 4.356740]\n",
            "291 [D loss: 0.089570, acc.: 95.31%] [G loss: 4.462819]\n",
            "292 [D loss: 0.103117, acc.: 96.88%] [G loss: 3.477819]\n",
            "293 [D loss: 0.143644, acc.: 98.44%] [G loss: 3.679933]\n",
            "294 [D loss: 0.132628, acc.: 96.88%] [G loss: 3.398390]\n",
            "295 [D loss: 0.479742, acc.: 76.56%] [G loss: 4.762336]\n",
            "296 [D loss: 0.288738, acc.: 90.62%] [G loss: 3.714159]\n",
            "297 [D loss: 0.815042, acc.: 68.75%] [G loss: 3.730982]\n",
            "298 [D loss: 0.378370, acc.: 87.50%] [G loss: 3.502674]\n",
            "299 [D loss: 0.482963, acc.: 84.38%] [G loss: 3.216881]\n",
            "300 [D loss: 0.447389, acc.: 87.50%] [G loss: 3.569805]\n",
            "generated_data\n",
            "301 [D loss: 0.469887, acc.: 87.50%] [G loss: 3.575065]\n",
            "302 [D loss: 0.340546, acc.: 90.62%] [G loss: 3.352751]\n",
            "303 [D loss: 0.443612, acc.: 87.50%] [G loss: 3.835830]\n",
            "304 [D loss: 0.327395, acc.: 90.62%] [G loss: 3.701805]\n",
            "305 [D loss: 0.356338, acc.: 92.19%] [G loss: 3.458509]\n",
            "306 [D loss: 0.377798, acc.: 87.50%] [G loss: 3.798856]\n",
            "307 [D loss: 0.322875, acc.: 90.62%] [G loss: 3.888659]\n",
            "308 [D loss: 0.354608, acc.: 87.50%] [G loss: 4.198100]\n",
            "309 [D loss: 0.279035, acc.: 87.50%] [G loss: 3.299149]\n",
            "310 [D loss: 0.409928, acc.: 87.50%] [G loss: 3.934740]\n",
            "311 [D loss: 0.292718, acc.: 87.50%] [G loss: 3.411572]\n",
            "312 [D loss: 0.491619, acc.: 78.12%] [G loss: 3.651775]\n",
            "313 [D loss: 0.311077, acc.: 90.62%] [G loss: 3.218830]\n",
            "314 [D loss: 0.399349, acc.: 85.94%] [G loss: 2.998002]\n",
            "315 [D loss: 0.330153, acc.: 90.62%] [G loss: 3.022811]\n",
            "316 [D loss: 0.339844, acc.: 92.19%] [G loss: 3.446604]\n",
            "317 [D loss: 0.337610, acc.: 92.19%] [G loss: 3.663783]\n",
            "318 [D loss: 0.359775, acc.: 87.50%] [G loss: 3.987786]\n",
            "319 [D loss: 0.300039, acc.: 93.75%] [G loss: 3.693947]\n",
            "320 [D loss: 0.261674, acc.: 92.19%] [G loss: 3.236083]\n",
            "321 [D loss: 0.213273, acc.: 92.19%] [G loss: 2.807795]\n",
            "322 [D loss: 0.293627, acc.: 89.06%] [G loss: 3.484015]\n",
            "323 [D loss: 0.137782, acc.: 90.62%] [G loss: 3.521560]\n",
            "324 [D loss: 0.178440, acc.: 93.75%] [G loss: 3.114625]\n",
            "325 [D loss: 0.181216, acc.: 92.19%] [G loss: 3.051270]\n",
            "326 [D loss: 0.136389, acc.: 93.75%] [G loss: 3.167755]\n",
            "327 [D loss: 0.150972, acc.: 95.31%] [G loss: 2.897307]\n",
            "328 [D loss: 0.173788, acc.: 93.75%] [G loss: 3.226989]\n",
            "329 [D loss: 0.135127, acc.: 95.31%] [G loss: 2.829831]\n",
            "330 [D loss: 0.155656, acc.: 93.75%] [G loss: 2.663575]\n",
            "331 [D loss: 0.169595, acc.: 95.31%] [G loss: 2.689982]\n",
            "332 [D loss: 0.182104, acc.: 95.31%] [G loss: 2.555110]\n",
            "333 [D loss: 0.184871, acc.: 95.31%] [G loss: 2.313945]\n",
            "334 [D loss: 0.240931, acc.: 92.19%] [G loss: 2.276350]\n",
            "335 [D loss: 0.352045, acc.: 87.50%] [G loss: 3.108171]\n",
            "336 [D loss: 0.242930, acc.: 92.19%] [G loss: 3.199814]\n",
            "337 [D loss: 0.353766, acc.: 85.94%] [G loss: 3.165859]\n",
            "338 [D loss: 0.263478, acc.: 92.19%] [G loss: 3.085323]\n",
            "339 [D loss: 0.378242, acc.: 87.50%] [G loss: 3.632271]\n",
            "340 [D loss: 0.261944, acc.: 93.75%] [G loss: 3.635836]\n",
            "341 [D loss: 0.307699, acc.: 89.06%] [G loss: 2.770691]\n",
            "342 [D loss: 0.406525, acc.: 85.94%] [G loss: 3.750844]\n",
            "343 [D loss: 0.188304, acc.: 93.75%] [G loss: 4.165921]\n",
            "344 [D loss: 0.211561, acc.: 90.62%] [G loss: 3.209583]\n",
            "345 [D loss: 0.209238, acc.: 92.19%] [G loss: 3.525118]\n",
            "346 [D loss: 0.131534, acc.: 95.31%] [G loss: 3.920527]\n",
            "347 [D loss: 0.129729, acc.: 95.31%] [G loss: 3.564328]\n",
            "348 [D loss: 0.180365, acc.: 89.06%] [G loss: 4.383374]\n",
            "349 [D loss: 0.108824, acc.: 96.88%] [G loss: 4.259585]\n",
            "350 [D loss: 0.154563, acc.: 95.31%] [G loss: 3.514198]\n",
            "351 [D loss: 0.105017, acc.: 98.44%] [G loss: 4.090716]\n",
            "352 [D loss: 0.130146, acc.: 95.31%] [G loss: 3.971431]\n",
            "353 [D loss: 0.145215, acc.: 96.88%] [G loss: 4.461973]\n",
            "354 [D loss: 0.077194, acc.: 98.44%] [G loss: 4.779447]\n",
            "355 [D loss: 0.151576, acc.: 95.31%] [G loss: 3.887504]\n",
            "356 [D loss: 0.118501, acc.: 96.88%] [G loss: 4.443148]\n",
            "357 [D loss: 0.072504, acc.: 96.88%] [G loss: 4.113607]\n",
            "358 [D loss: 0.137052, acc.: 96.88%] [G loss: 4.428689]\n",
            "359 [D loss: 0.090752, acc.: 93.75%] [G loss: 4.302108]\n",
            "360 [D loss: 0.207124, acc.: 93.75%] [G loss: 3.799044]\n",
            "361 [D loss: 0.208199, acc.: 93.75%] [G loss: 3.737852]\n",
            "362 [D loss: 0.279133, acc.: 93.75%] [G loss: 4.541344]\n",
            "363 [D loss: 0.270446, acc.: 93.75%] [G loss: 3.409152]\n",
            "364 [D loss: 0.404826, acc.: 89.06%] [G loss: 3.900885]\n",
            "365 [D loss: 0.278496, acc.: 93.75%] [G loss: 3.800066]\n",
            "366 [D loss: 0.297655, acc.: 92.19%] [G loss: 2.861304]\n",
            "367 [D loss: 0.224107, acc.: 95.31%] [G loss: 3.137338]\n",
            "368 [D loss: 0.220866, acc.: 93.75%] [G loss: 2.665115]\n",
            "369 [D loss: 0.214525, acc.: 93.75%] [G loss: 2.748738]\n",
            "370 [D loss: 0.133123, acc.: 96.88%] [G loss: 3.073027]\n",
            "371 [D loss: 0.135137, acc.: 96.88%] [G loss: 3.044434]\n",
            "372 [D loss: 0.129931, acc.: 96.88%] [G loss: 2.678196]\n",
            "373 [D loss: 0.192865, acc.: 93.75%] [G loss: 2.962370]\n",
            "374 [D loss: 0.149831, acc.: 95.31%] [G loss: 3.265697]\n",
            "375 [D loss: 0.214723, acc.: 92.19%] [G loss: 3.895667]\n",
            "376 [D loss: 0.181657, acc.: 95.31%] [G loss: 3.670524]\n",
            "377 [D loss: 0.117831, acc.: 98.44%] [G loss: 2.969568]\n",
            "378 [D loss: 0.139483, acc.: 98.44%] [G loss: 2.227402]\n",
            "379 [D loss: 0.264844, acc.: 90.62%] [G loss: 2.211497]\n",
            "380 [D loss: 0.217267, acc.: 90.62%] [G loss: 2.473970]\n",
            "381 [D loss: 0.198568, acc.: 93.75%] [G loss: 2.950711]\n",
            "382 [D loss: 0.136957, acc.: 95.31%] [G loss: 2.948433]\n",
            "383 [D loss: 0.140935, acc.: 93.75%] [G loss: 2.964179]\n",
            "384 [D loss: 0.170858, acc.: 95.31%] [G loss: 3.000842]\n",
            "385 [D loss: 0.171351, acc.: 95.31%] [G loss: 2.653086]\n",
            "386 [D loss: 0.228528, acc.: 93.75%] [G loss: 2.714272]\n",
            "387 [D loss: 0.201931, acc.: 95.31%] [G loss: 2.770774]\n",
            "388 [D loss: 0.237389, acc.: 90.62%] [G loss: 3.118097]\n",
            "389 [D loss: 0.147398, acc.: 93.75%] [G loss: 3.141630]\n",
            "390 [D loss: 0.182615, acc.: 92.19%] [G loss: 2.924800]\n",
            "391 [D loss: 0.189250, acc.: 95.31%] [G loss: 3.217609]\n",
            "392 [D loss: 0.134277, acc.: 95.31%] [G loss: 3.424436]\n",
            "393 [D loss: 0.158879, acc.: 95.31%] [G loss: 3.237046]\n",
            "394 [D loss: 0.137037, acc.: 93.75%] [G loss: 3.118567]\n",
            "395 [D loss: 0.124627, acc.: 95.31%] [G loss: 3.159524]\n",
            "396 [D loss: 0.111820, acc.: 96.88%] [G loss: 3.373105]\n",
            "397 [D loss: 0.123874, acc.: 96.88%] [G loss: 3.661971]\n",
            "398 [D loss: 0.123819, acc.: 95.31%] [G loss: 3.404288]\n",
            "399 [D loss: 0.185082, acc.: 95.31%] [G loss: 3.596467]\n",
            "400 [D loss: 0.195096, acc.: 93.75%] [G loss: 3.518038]\n",
            "generated_data\n",
            "401 [D loss: 0.220512, acc.: 95.31%] [G loss: 4.129388]\n",
            "402 [D loss: 0.213392, acc.: 95.31%] [G loss: 3.590828]\n",
            "403 [D loss: 0.213067, acc.: 92.19%] [G loss: 3.439989]\n",
            "404 [D loss: 0.268249, acc.: 93.75%] [G loss: 4.846432]\n",
            "405 [D loss: 0.214601, acc.: 96.88%] [G loss: 4.440492]\n",
            "406 [D loss: 0.242857, acc.: 95.31%] [G loss: 3.609942]\n",
            "407 [D loss: 0.261441, acc.: 92.19%] [G loss: 4.298212]\n",
            "408 [D loss: 0.187377, acc.: 95.31%] [G loss: 4.111735]\n",
            "409 [D loss: 0.246449, acc.: 95.31%] [G loss: 3.571824]\n",
            "410 [D loss: 0.201988, acc.: 93.75%] [G loss: 4.289631]\n",
            "411 [D loss: 0.180016, acc.: 95.31%] [G loss: 4.298957]\n",
            "412 [D loss: 0.204706, acc.: 92.19%] [G loss: 4.754694]\n",
            "413 [D loss: 0.103269, acc.: 95.31%] [G loss: 4.279736]\n",
            "414 [D loss: 0.120545, acc.: 93.75%] [G loss: 3.887637]\n",
            "415 [D loss: 0.083256, acc.: 98.44%] [G loss: 3.889227]\n",
            "416 [D loss: 0.082735, acc.: 98.44%] [G loss: 3.796406]\n",
            "417 [D loss: 0.079750, acc.: 98.44%] [G loss: 4.013986]\n",
            "418 [D loss: 0.071850, acc.: 98.44%] [G loss: 3.517088]\n",
            "419 [D loss: 0.348794, acc.: 81.25%] [G loss: 5.082521]\n",
            "420 [D loss: 0.152609, acc.: 92.19%] [G loss: 4.767041]\n",
            "421 [D loss: 0.110559, acc.: 96.88%] [G loss: 3.081874]\n",
            "422 [D loss: 0.224237, acc.: 92.19%] [G loss: 2.981307]\n",
            "423 [D loss: 0.166078, acc.: 96.88%] [G loss: 2.955879]\n",
            "424 [D loss: 0.263806, acc.: 93.75%] [G loss: 3.769389]\n",
            "425 [D loss: 0.131442, acc.: 95.31%] [G loss: 3.810785]\n",
            "426 [D loss: 0.131106, acc.: 96.88%] [G loss: 3.696157]\n",
            "427 [D loss: 0.121910, acc.: 96.88%] [G loss: 3.170115]\n",
            "428 [D loss: 0.149246, acc.: 98.44%] [G loss: 2.879786]\n",
            "429 [D loss: 0.201631, acc.: 90.62%] [G loss: 3.440924]\n",
            "430 [D loss: 0.167094, acc.: 93.75%] [G loss: 3.532347]\n",
            "431 [D loss: 0.247889, acc.: 90.62%] [G loss: 3.084250]\n",
            "432 [D loss: 0.210937, acc.: 93.75%] [G loss: 2.987411]\n",
            "433 [D loss: 0.279276, acc.: 90.62%] [G loss: 3.387805]\n",
            "434 [D loss: 0.230138, acc.: 93.75%] [G loss: 3.143643]\n",
            "435 [D loss: 0.239140, acc.: 93.75%] [G loss: 2.689133]\n",
            "436 [D loss: 0.222268, acc.: 93.75%] [G loss: 2.970675]\n",
            "437 [D loss: 0.202530, acc.: 93.75%] [G loss: 3.001529]\n",
            "438 [D loss: 0.156168, acc.: 95.31%] [G loss: 2.956633]\n",
            "439 [D loss: 0.128385, acc.: 95.31%] [G loss: 3.052867]\n",
            "440 [D loss: 0.119904, acc.: 96.88%] [G loss: 2.972339]\n",
            "441 [D loss: 0.126242, acc.: 96.88%] [G loss: 3.083036]\n",
            "442 [D loss: 0.161702, acc.: 96.88%] [G loss: 3.308577]\n",
            "443 [D loss: 0.122126, acc.: 98.44%] [G loss: 3.545437]\n",
            "444 [D loss: 0.189763, acc.: 92.19%] [G loss: 3.768433]\n",
            "445 [D loss: 0.138856, acc.: 93.75%] [G loss: 3.505740]\n",
            "446 [D loss: 0.156602, acc.: 95.31%] [G loss: 2.857060]\n",
            "447 [D loss: 0.244082, acc.: 90.62%] [G loss: 3.308637]\n",
            "448 [D loss: 0.116888, acc.: 93.75%] [G loss: 3.515234]\n",
            "449 [D loss: 0.147354, acc.: 93.75%] [G loss: 3.040704]\n",
            "450 [D loss: 0.144208, acc.: 95.31%] [G loss: 3.285412]\n",
            "451 [D loss: 0.135966, acc.: 93.75%] [G loss: 3.511707]\n",
            "452 [D loss: 0.144034, acc.: 93.75%] [G loss: 3.405281]\n",
            "453 [D loss: 0.164919, acc.: 92.19%] [G loss: 4.147298]\n",
            "454 [D loss: 0.110106, acc.: 95.31%] [G loss: 3.992078]\n",
            "455 [D loss: 0.131397, acc.: 96.88%] [G loss: 3.458145]\n",
            "456 [D loss: 0.160705, acc.: 95.31%] [G loss: 3.544995]\n",
            "457 [D loss: 0.163529, acc.: 95.31%] [G loss: 3.906345]\n",
            "458 [D loss: 0.147827, acc.: 96.88%] [G loss: 3.646508]\n",
            "459 [D loss: 0.239265, acc.: 92.19%] [G loss: 3.899577]\n",
            "460 [D loss: 0.201027, acc.: 95.31%] [G loss: 3.608684]\n",
            "461 [D loss: 0.253414, acc.: 93.75%] [G loss: 3.756449]\n",
            "462 [D loss: 0.190106, acc.: 95.31%] [G loss: 3.375294]\n",
            "463 [D loss: 0.276219, acc.: 90.62%] [G loss: 3.995343]\n",
            "464 [D loss: 0.212761, acc.: 93.75%] [G loss: 3.306666]\n",
            "465 [D loss: 0.236112, acc.: 95.31%] [G loss: 3.200794]\n",
            "466 [D loss: 0.268347, acc.: 90.62%] [G loss: 3.091623]\n",
            "467 [D loss: 0.212533, acc.: 93.75%] [G loss: 3.289734]\n",
            "468 [D loss: 0.214497, acc.: 92.19%] [G loss: 3.010761]\n",
            "469 [D loss: 0.213203, acc.: 93.75%] [G loss: 3.179354]\n",
            "470 [D loss: 0.200290, acc.: 95.31%] [G loss: 3.480169]\n",
            "471 [D loss: 0.202886, acc.: 95.31%] [G loss: 3.083736]\n",
            "472 [D loss: 0.218132, acc.: 95.31%] [G loss: 3.160289]\n",
            "473 [D loss: 0.206017, acc.: 93.75%] [G loss: 3.291995]\n",
            "474 [D loss: 0.237152, acc.: 92.19%] [G loss: 3.428410]\n",
            "475 [D loss: 0.203771, acc.: 95.31%] [G loss: 3.144705]\n",
            "476 [D loss: 0.203106, acc.: 93.75%] [G loss: 3.135120]\n",
            "477 [D loss: 0.194165, acc.: 95.31%] [G loss: 2.970821]\n",
            "478 [D loss: 0.172661, acc.: 95.31%] [G loss: 3.054267]\n",
            "479 [D loss: 0.173229, acc.: 95.31%] [G loss: 2.821993]\n",
            "480 [D loss: 0.186387, acc.: 93.75%] [G loss: 2.762280]\n",
            "481 [D loss: 0.158449, acc.: 96.88%] [G loss: 2.871331]\n",
            "482 [D loss: 0.139289, acc.: 95.31%] [G loss: 2.870199]\n",
            "483 [D loss: 0.129869, acc.: 96.88%] [G loss: 2.987806]\n",
            "484 [D loss: 0.117408, acc.: 96.88%] [G loss: 2.966465]\n",
            "485 [D loss: 0.121382, acc.: 95.31%] [G loss: 2.941596]\n",
            "486 [D loss: 0.102920, acc.: 96.88%] [G loss: 2.765688]\n",
            "487 [D loss: 0.126590, acc.: 95.31%] [G loss: 2.576110]\n",
            "488 [D loss: 0.141678, acc.: 95.31%] [G loss: 2.684111]\n",
            "489 [D loss: 0.151123, acc.: 93.75%] [G loss: 2.971695]\n",
            "490 [D loss: 0.104057, acc.: 98.44%] [G loss: 2.976695]\n",
            "491 [D loss: 0.157667, acc.: 93.75%] [G loss: 2.697378]\n",
            "492 [D loss: 0.272660, acc.: 89.06%] [G loss: 2.768146]\n",
            "493 [D loss: 0.172426, acc.: 95.31%] [G loss: 2.766593]\n",
            "494 [D loss: 0.307042, acc.: 84.38%] [G loss: 3.459018]\n",
            "495 [D loss: 0.197325, acc.: 93.75%] [G loss: 3.629925]\n",
            "496 [D loss: 0.181990, acc.: 93.75%] [G loss: 3.383309]\n",
            "497 [D loss: 0.288657, acc.: 89.06%] [G loss: 4.043159]\n",
            "498 [D loss: 0.190833, acc.: 92.19%] [G loss: 4.308512]\n",
            "499 [D loss: 0.140779, acc.: 96.88%] [G loss: 3.643913]\n",
            "500 [D loss: 0.163090, acc.: 93.75%] [G loss: 3.539798]\n",
            "generated_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWFeJe6qGoDE"
      },
      "source": [
        "After, 5000 epochs the models shows a training accuracy of 95.31% which sounds quite impressive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYc0H-PuJ7lt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc5293c2-3153-4d17-92d0-98a93906164a"
      },
      "source": [
        "!mkdir model/gan"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘model/gan’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzntybfRyD1k"
      },
      "source": [
        "#You can easily save the trained generator and loaded it aftwerwards\n",
        "\n",
        "synthesizer.save('model/gan/saved', 'generator_patients')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BeswxI6IAI0"
      },
      "source": [
        "Let's take a look at the Generator and Discriminator models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrzrZ7ioz3sx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0bbdad-2432-47d7-dfea-2b7c560cd015"
      },
      "source": [
        "synthesizer.generator.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_8 (InputLayer)            [(32, 32)]           0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_9 (InputLayer)            [(32, 1)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_10 (InputLayer)           [(32, 1)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_10 (Dense)                (32, 128)            4224        input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_11 (Dense)                (32, 128)            256         input_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (32, 128)            256         input_10[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (32, 384)            0           dense_10[0][0]                   \n",
            "                                                                 dense_11[0][0]                   \n",
            "                                                                 dense_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (32, 256)            98560       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_14 (Dense)                (32, 512)            131584      dense_13[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (32, 13)             6669        dense_14[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 241,549\n",
            "Trainable params: 241,549\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08wQukVHz5fi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e7b4611-bc92-426d-a1df-e5a5509763f6"
      },
      "source": [
        "synthesizer.discriminator.summary()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_11 (InputLayer)        [(32, 13)]                0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (32, 512)                 7168      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (32, 512)                 0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (32, 256)                 131328    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (32, 256)                 0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (32, 128)                 32896     \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (32, 1)                   129       \n",
            "=================================================================\n",
            "Total params: 171,521\n",
            "Trainable params: 0\n",
            "Non-trainable params: 171,521\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKD1WYVHH6ID"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Now, that we have trained the model let's see if the generated data is similar to the actual data. \n",
        "\n",
        "We plot the generated data for some of the model steps and see how the plot for the generated data changes as the networks learns the embedding more accurately. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVPhtlTQz7PM"
      },
      "source": [
        "models = {'GAN': ['GAN', False, synthesizer.generator]}"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DxCIbS9z9Xi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "33a7fa8d-5770-46fb-c3f4-4ce10eface35"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Setup parameters visualization parameters\n",
        "seed = 17\n",
        "test_size = 1000 # number of fraud cases\n",
        "noise_dim = 32\n",
        "\n",
        "np.random.seed(seed)\n",
        "z = np.random.normal(size=(test_size, noise_dim))\n",
        "y = np.random.random_integers(0, 1,test_size)\n",
        "s = np.random.random_integers(0, 1,test_size)\n",
        "\n",
        "real = synthesizer.get_data_batch(train=df, batch_size=test_size, seed=seed)\n",
        "real_samples = pd.DataFrame(real, columns=data_cols)\n",
        "\n",
        "model_names = ['GAN']\n",
        "colors = ['deepskyblue','blue']\n",
        "markers = ['o','^']\n",
        "\n",
        "base_dir = 'model/'\n",
        "\n",
        "#Actual fraud data visualization\n",
        "model_steps = [ 0, 100, 200, 300, 400, 500, 1000, 2000, 3000, 4000, 5000]\n",
        "rows = len(model_steps)\n",
        "columns = 5\n",
        "\n",
        "axarr = [[]]*len(model_steps)\n",
        "\n",
        "fig = plt.figure(figsize=(14,rows*3))\n",
        "\n",
        "for model_step_ix, model_step in enumerate(model_steps):        \n",
        "    axarr[model_step_ix] = plt.subplot(rows, columns, model_step_ix*columns + 1)\n",
        "    \n",
        "    for group, color, marker in zip(real_samples.groupby(col_group_by), colors, markers):\n",
        "        plt.scatter( group[1][[col1]], group[1][[col2]], marker=marker, edgecolors=color, facecolors='none' )\n",
        "    \n",
        "    plt.title('Actual Patients Data')\n",
        "    plt.ylabel(col2) # Only add y label to left plot\n",
        "    plt.xlabel(col1)\n",
        "    xlims, ylims = axarr[model_step_ix].get_xlim(), axarr[model_step_ix].get_ylim()\n",
        "    \n",
        "    if model_step_ix == 0: \n",
        "        legend = plt.legend()\n",
        "        legend.get_frame().set_facecolor('white')\n",
        "    \n",
        "    i=0\n",
        "    [model_name, with_class, generator_model] = models['GAN']\n",
        "\n",
        "    generator_model.load_weights( base_dir + '_generator_model_weights_step_'+str(model_step)+'.h5')\n",
        "\n",
        "    ax = plt.subplot(rows, columns, model_step_ix*columns + 1 + (i+1) )\n",
        "\n",
        "    g_z = generator_model.predict([z, y, s])\n",
        "\n",
        "    gen_samples = pd.DataFrame(g_z, columns=data_cols)\n",
        "    gen_samples.to_csv('Generated_sample.csv')\n",
        "    plt.scatter( gen_samples[[col1]], gen_samples[[col2]], marker=markers[0], edgecolors=colors[0], facecolors='none' )\n",
        "    plt.title(\"Generated Data\")   \n",
        "    plt.xlabel(data_cols[0])\n",
        "    ax.set_xlim(xlims), ax.set_ylim(ylims)\n",
        "\n",
        "plt.suptitle('Comparison of GAN outputs', size=16, fontweight='bold')\n",
        "plt.tight_layout(rect=[0.075,0,1,0.95])\n",
        "\n",
        "# Adding text labels for traning steps\n",
        "vpositions = np.array([ i._position.bounds[1] for i in axarr ])\n",
        "vpositions += ((vpositions[0] - vpositions[1]) * 0.35 )\n",
        "for model_step_ix, model_step in enumerate( model_steps ):\n",
        "    fig.text( 0.05, vpositions[model_step_ix], 'training\\nstep\\n'+str(model_step), ha='center', va='center', size=12)\n",
        "\n",
        "plt.savefig('Comparison_of_GAN_outputs.png')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: This function is deprecated. Please call randint(0, 1 + 1) instead\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: This function is deprecated. Please call randint(0, 1 + 1) instead\n",
            "  # This is added back by InteractiveShellApp.init_path()\n",
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-c44235877cb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_model\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'GAN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mgenerator_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_generator_model_weights_step_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_step_ix\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2317\u001b[0m           'first, then load the weights.')\n\u001b[1;32m   2318\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2320\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2321\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001b[0m\n\u001b[1;32m    425\u001b[0m                                fapl, fcpl=make_fcpl(track_order=track_order, fs_strategy=fs_strategy,\n\u001b[1;32m    426\u001b[0m                                fs_persist=fs_persist, fs_threshold=fs_threshold),\n\u001b[0;32m--> 427\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'model/_generator_model_weights_step_1000.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x2376 with 13 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G0YnihkJmom"
      },
      "source": [
        "g_z=pw.inverse_transform(g_z)\n",
        "gen_samples = pd.DataFrame(g_z, columns=data_cols)\n",
        "gen_samples.to_csv('Generated_sample.csv')"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh1C7nW3cMHb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b152e583-e824-426b-bb88-9006df849c2c"
      },
      "source": [
        "gen_samples[:50]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Workclass</th>\n",
              "      <th>Education</th>\n",
              "      <th>Martial Status</th>\n",
              "      <th>Occupation</th>\n",
              "      <th>Relationship</th>\n",
              "      <th>Race</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Country</th>\n",
              "      <th>Target</th>\n",
              "      <th>Age</th>\n",
              "      <th>Capital Gain</th>\n",
              "      <th>Capital Loss</th>\n",
              "      <th>Hours per week</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.487170</td>\n",
              "      <td>7.725574</td>\n",
              "      <td>1.712002</td>\n",
              "      <td>4.856231</td>\n",
              "      <td>0.088017</td>\n",
              "      <td>3.985737</td>\n",
              "      <td>0.817282</td>\n",
              "      <td>37.798653</td>\n",
              "      <td>0.066753</td>\n",
              "      <td>6.945696</td>\n",
              "      <td>0.000386</td>\n",
              "      <td>-0.001699</td>\n",
              "      <td>5.952133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.713927</td>\n",
              "      <td>8.978780</td>\n",
              "      <td>2.111154</td>\n",
              "      <td>5.067421</td>\n",
              "      <td>0.278312</td>\n",
              "      <td>3.970114</td>\n",
              "      <td>0.821541</td>\n",
              "      <td>37.776279</td>\n",
              "      <td>0.054315</td>\n",
              "      <td>6.199500</td>\n",
              "      <td>0.001607</td>\n",
              "      <td>0.001573</td>\n",
              "      <td>6.886689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.938487</td>\n",
              "      <td>4.035961</td>\n",
              "      <td>0.848258</td>\n",
              "      <td>4.424970</td>\n",
              "      <td>-0.239293</td>\n",
              "      <td>4.030900</td>\n",
              "      <td>0.791043</td>\n",
              "      <td>37.776108</td>\n",
              "      <td>0.090103</td>\n",
              "      <td>8.734258</td>\n",
              "      <td>-0.002064</td>\n",
              "      <td>-0.007605</td>\n",
              "      <td>3.985677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.530607</td>\n",
              "      <td>7.986984</td>\n",
              "      <td>1.832887</td>\n",
              "      <td>4.908464</td>\n",
              "      <td>0.148851</td>\n",
              "      <td>3.981620</td>\n",
              "      <td>0.809199</td>\n",
              "      <td>37.753891</td>\n",
              "      <td>0.056795</td>\n",
              "      <td>6.776282</td>\n",
              "      <td>0.000765</td>\n",
              "      <td>-0.000561</td>\n",
              "      <td>6.197436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.713759</td>\n",
              "      <td>8.982026</td>\n",
              "      <td>2.071951</td>\n",
              "      <td>5.026451</td>\n",
              "      <td>0.277457</td>\n",
              "      <td>3.969180</td>\n",
              "      <td>0.816173</td>\n",
              "      <td>37.765511</td>\n",
              "      <td>0.056480</td>\n",
              "      <td>6.174615</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001590</td>\n",
              "      <td>6.938636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1.730287</td>\n",
              "      <td>8.976460</td>\n",
              "      <td>2.130299</td>\n",
              "      <td>5.113085</td>\n",
              "      <td>0.305577</td>\n",
              "      <td>3.968311</td>\n",
              "      <td>0.811499</td>\n",
              "      <td>37.757942</td>\n",
              "      <td>0.056991</td>\n",
              "      <td>6.196667</td>\n",
              "      <td>0.001604</td>\n",
              "      <td>0.001370</td>\n",
              "      <td>6.820226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.595349</td>\n",
              "      <td>8.235085</td>\n",
              "      <td>1.942714</td>\n",
              "      <td>4.989317</td>\n",
              "      <td>0.178286</td>\n",
              "      <td>3.978288</td>\n",
              "      <td>0.815177</td>\n",
              "      <td>37.757191</td>\n",
              "      <td>0.057907</td>\n",
              "      <td>6.583991</td>\n",
              "      <td>0.001046</td>\n",
              "      <td>-0.000100</td>\n",
              "      <td>6.316930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1.579782</td>\n",
              "      <td>8.321161</td>\n",
              "      <td>1.935912</td>\n",
              "      <td>5.036506</td>\n",
              "      <td>0.193492</td>\n",
              "      <td>3.975622</td>\n",
              "      <td>0.817848</td>\n",
              "      <td>37.771645</td>\n",
              "      <td>0.061020</td>\n",
              "      <td>6.539076</td>\n",
              "      <td>0.000861</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>6.427892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1.614259</td>\n",
              "      <td>8.551085</td>\n",
              "      <td>1.963260</td>\n",
              "      <td>4.957094</td>\n",
              "      <td>0.193391</td>\n",
              "      <td>3.976187</td>\n",
              "      <td>0.821424</td>\n",
              "      <td>37.784595</td>\n",
              "      <td>0.060713</td>\n",
              "      <td>6.447925</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.000347</td>\n",
              "      <td>6.532008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.872945</td>\n",
              "      <td>9.661609</td>\n",
              "      <td>2.295867</td>\n",
              "      <td>5.170289</td>\n",
              "      <td>0.448898</td>\n",
              "      <td>3.955529</td>\n",
              "      <td>0.813922</td>\n",
              "      <td>37.775059</td>\n",
              "      <td>0.053749</td>\n",
              "      <td>5.701827</td>\n",
              "      <td>0.002243</td>\n",
              "      <td>0.003065</td>\n",
              "      <td>7.394825</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.724890</td>\n",
              "      <td>9.011563</td>\n",
              "      <td>2.116237</td>\n",
              "      <td>5.094055</td>\n",
              "      <td>0.304089</td>\n",
              "      <td>3.967452</td>\n",
              "      <td>0.814587</td>\n",
              "      <td>37.759235</td>\n",
              "      <td>0.056870</td>\n",
              "      <td>6.169247</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.001496</td>\n",
              "      <td>6.800772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1.583834</td>\n",
              "      <td>8.280060</td>\n",
              "      <td>1.889637</td>\n",
              "      <td>4.859170</td>\n",
              "      <td>0.159665</td>\n",
              "      <td>3.980220</td>\n",
              "      <td>0.816116</td>\n",
              "      <td>37.751060</td>\n",
              "      <td>0.056926</td>\n",
              "      <td>6.623483</td>\n",
              "      <td>0.000742</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>6.282540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.951133</td>\n",
              "      <td>4.270750</td>\n",
              "      <td>0.890591</td>\n",
              "      <td>4.477577</td>\n",
              "      <td>-0.206024</td>\n",
              "      <td>4.018335</td>\n",
              "      <td>0.799640</td>\n",
              "      <td>37.738991</td>\n",
              "      <td>0.085501</td>\n",
              "      <td>8.765403</td>\n",
              "      <td>-0.002284</td>\n",
              "      <td>-0.007312</td>\n",
              "      <td>3.893327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.537444</td>\n",
              "      <td>-2.218637</td>\n",
              "      <td>0.164408</td>\n",
              "      <td>4.046290</td>\n",
              "      <td>-0.458927</td>\n",
              "      <td>4.060039</td>\n",
              "      <td>0.796832</td>\n",
              "      <td>37.766212</td>\n",
              "      <td>0.108873</td>\n",
              "      <td>10.552790</td>\n",
              "      <td>-0.004315</td>\n",
              "      <td>-0.012421</td>\n",
              "      <td>2.094334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1.605082</td>\n",
              "      <td>8.440021</td>\n",
              "      <td>1.921758</td>\n",
              "      <td>4.972905</td>\n",
              "      <td>0.191552</td>\n",
              "      <td>3.978042</td>\n",
              "      <td>0.816082</td>\n",
              "      <td>37.786610</td>\n",
              "      <td>0.056992</td>\n",
              "      <td>6.474675</td>\n",
              "      <td>0.001143</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>6.563414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1.465360</td>\n",
              "      <td>7.668321</td>\n",
              "      <td>1.755211</td>\n",
              "      <td>4.916940</td>\n",
              "      <td>0.064205</td>\n",
              "      <td>3.986103</td>\n",
              "      <td>0.824090</td>\n",
              "      <td>37.769417</td>\n",
              "      <td>0.062101</td>\n",
              "      <td>6.908051</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>-0.001256</td>\n",
              "      <td>5.922861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1.606985</td>\n",
              "      <td>8.414943</td>\n",
              "      <td>1.921833</td>\n",
              "      <td>4.937845</td>\n",
              "      <td>0.179553</td>\n",
              "      <td>3.976797</td>\n",
              "      <td>0.815873</td>\n",
              "      <td>37.777084</td>\n",
              "      <td>0.061404</td>\n",
              "      <td>6.415679</td>\n",
              "      <td>0.001019</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>6.528314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.146590</td>\n",
              "      <td>5.621670</td>\n",
              "      <td>1.268802</td>\n",
              "      <td>4.568346</td>\n",
              "      <td>-0.151394</td>\n",
              "      <td>4.014315</td>\n",
              "      <td>0.821751</td>\n",
              "      <td>37.770325</td>\n",
              "      <td>0.071090</td>\n",
              "      <td>7.954752</td>\n",
              "      <td>-0.001117</td>\n",
              "      <td>-0.005674</td>\n",
              "      <td>4.643296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1.001755</td>\n",
              "      <td>4.573864</td>\n",
              "      <td>0.987808</td>\n",
              "      <td>4.473349</td>\n",
              "      <td>-0.199665</td>\n",
              "      <td>4.020076</td>\n",
              "      <td>0.807715</td>\n",
              "      <td>37.755547</td>\n",
              "      <td>0.084272</td>\n",
              "      <td>8.580368</td>\n",
              "      <td>-0.002361</td>\n",
              "      <td>-0.007684</td>\n",
              "      <td>3.937124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1.611728</td>\n",
              "      <td>8.431912</td>\n",
              "      <td>1.949518</td>\n",
              "      <td>5.002292</td>\n",
              "      <td>0.212215</td>\n",
              "      <td>3.975871</td>\n",
              "      <td>0.815353</td>\n",
              "      <td>37.784012</td>\n",
              "      <td>0.060678</td>\n",
              "      <td>6.431442</td>\n",
              "      <td>0.001304</td>\n",
              "      <td>0.000206</td>\n",
              "      <td>6.602500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.049513</td>\n",
              "      <td>5.023397</td>\n",
              "      <td>1.015556</td>\n",
              "      <td>4.415126</td>\n",
              "      <td>-0.178565</td>\n",
              "      <td>4.015640</td>\n",
              "      <td>0.806033</td>\n",
              "      <td>37.784988</td>\n",
              "      <td>0.081360</td>\n",
              "      <td>8.171123</td>\n",
              "      <td>-0.001693</td>\n",
              "      <td>-0.006330</td>\n",
              "      <td>4.393504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1.427069</td>\n",
              "      <td>7.501954</td>\n",
              "      <td>1.660482</td>\n",
              "      <td>4.731136</td>\n",
              "      <td>0.032397</td>\n",
              "      <td>3.992838</td>\n",
              "      <td>0.822012</td>\n",
              "      <td>37.787506</td>\n",
              "      <td>0.067681</td>\n",
              "      <td>6.964905</td>\n",
              "      <td>0.000331</td>\n",
              "      <td>-0.001646</td>\n",
              "      <td>5.797344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.910712</td>\n",
              "      <td>4.028115</td>\n",
              "      <td>0.854483</td>\n",
              "      <td>4.436147</td>\n",
              "      <td>-0.222900</td>\n",
              "      <td>4.023287</td>\n",
              "      <td>0.803092</td>\n",
              "      <td>37.766315</td>\n",
              "      <td>0.089632</td>\n",
              "      <td>8.878559</td>\n",
              "      <td>-0.002431</td>\n",
              "      <td>-0.007954</td>\n",
              "      <td>3.705495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1.577421</td>\n",
              "      <td>8.443593</td>\n",
              "      <td>1.934500</td>\n",
              "      <td>4.959092</td>\n",
              "      <td>0.158139</td>\n",
              "      <td>3.984169</td>\n",
              "      <td>0.827862</td>\n",
              "      <td>37.799759</td>\n",
              "      <td>0.058614</td>\n",
              "      <td>6.450674</td>\n",
              "      <td>0.000863</td>\n",
              "      <td>-0.000008</td>\n",
              "      <td>6.496691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1.418356</td>\n",
              "      <td>7.473160</td>\n",
              "      <td>1.679975</td>\n",
              "      <td>4.869182</td>\n",
              "      <td>0.065194</td>\n",
              "      <td>3.989444</td>\n",
              "      <td>0.813922</td>\n",
              "      <td>37.732227</td>\n",
              "      <td>0.063688</td>\n",
              "      <td>7.121783</td>\n",
              "      <td>0.000253</td>\n",
              "      <td>-0.001437</td>\n",
              "      <td>5.829005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1.291739</td>\n",
              "      <td>6.907618</td>\n",
              "      <td>1.505287</td>\n",
              "      <td>4.617299</td>\n",
              "      <td>-0.065356</td>\n",
              "      <td>4.004432</td>\n",
              "      <td>0.834534</td>\n",
              "      <td>37.808250</td>\n",
              "      <td>0.063005</td>\n",
              "      <td>7.444071</td>\n",
              "      <td>-0.000335</td>\n",
              "      <td>-0.003361</td>\n",
              "      <td>5.488407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1.773343</td>\n",
              "      <td>9.287158</td>\n",
              "      <td>2.192336</td>\n",
              "      <td>5.094464</td>\n",
              "      <td>0.346853</td>\n",
              "      <td>3.962844</td>\n",
              "      <td>0.817615</td>\n",
              "      <td>37.772331</td>\n",
              "      <td>0.052645</td>\n",
              "      <td>6.014645</td>\n",
              "      <td>0.001892</td>\n",
              "      <td>0.002123</td>\n",
              "      <td>7.124039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1.818380</td>\n",
              "      <td>9.419474</td>\n",
              "      <td>2.239143</td>\n",
              "      <td>5.154436</td>\n",
              "      <td>0.400096</td>\n",
              "      <td>3.958570</td>\n",
              "      <td>0.815136</td>\n",
              "      <td>37.769878</td>\n",
              "      <td>0.054601</td>\n",
              "      <td>5.852939</td>\n",
              "      <td>0.001981</td>\n",
              "      <td>0.002574</td>\n",
              "      <td>7.219224</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1.439776</td>\n",
              "      <td>7.426140</td>\n",
              "      <td>1.691907</td>\n",
              "      <td>4.831957</td>\n",
              "      <td>0.079961</td>\n",
              "      <td>3.985809</td>\n",
              "      <td>0.804706</td>\n",
              "      <td>37.784561</td>\n",
              "      <td>0.063842</td>\n",
              "      <td>6.945414</td>\n",
              "      <td>0.000140</td>\n",
              "      <td>-0.002033</td>\n",
              "      <td>5.832234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1.568415</td>\n",
              "      <td>8.339447</td>\n",
              "      <td>1.859778</td>\n",
              "      <td>4.990997</td>\n",
              "      <td>0.173944</td>\n",
              "      <td>3.981054</td>\n",
              "      <td>0.816165</td>\n",
              "      <td>37.757694</td>\n",
              "      <td>0.061770</td>\n",
              "      <td>6.654584</td>\n",
              "      <td>0.001113</td>\n",
              "      <td>-0.000137</td>\n",
              "      <td>6.385326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1.380296</td>\n",
              "      <td>7.256145</td>\n",
              "      <td>1.558092</td>\n",
              "      <td>4.747221</td>\n",
              "      <td>0.027686</td>\n",
              "      <td>3.991459</td>\n",
              "      <td>0.812712</td>\n",
              "      <td>37.757008</td>\n",
              "      <td>0.067766</td>\n",
              "      <td>7.210969</td>\n",
              "      <td>-0.000008</td>\n",
              "      <td>-0.002516</td>\n",
              "      <td>5.591858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1.756653</td>\n",
              "      <td>9.139848</td>\n",
              "      <td>2.157981</td>\n",
              "      <td>5.096423</td>\n",
              "      <td>0.348001</td>\n",
              "      <td>3.967071</td>\n",
              "      <td>0.815647</td>\n",
              "      <td>37.778999</td>\n",
              "      <td>0.052681</td>\n",
              "      <td>5.995047</td>\n",
              "      <td>0.001819</td>\n",
              "      <td>0.001967</td>\n",
              "      <td>7.068528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1.619518</td>\n",
              "      <td>8.483880</td>\n",
              "      <td>2.003468</td>\n",
              "      <td>5.001972</td>\n",
              "      <td>0.224411</td>\n",
              "      <td>3.975451</td>\n",
              "      <td>0.814273</td>\n",
              "      <td>37.776062</td>\n",
              "      <td>0.059635</td>\n",
              "      <td>6.448482</td>\n",
              "      <td>0.001125</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>6.503043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1.792905</td>\n",
              "      <td>9.419403</td>\n",
              "      <td>2.226597</td>\n",
              "      <td>5.121783</td>\n",
              "      <td>0.370642</td>\n",
              "      <td>3.962412</td>\n",
              "      <td>0.820093</td>\n",
              "      <td>37.763268</td>\n",
              "      <td>0.053299</td>\n",
              "      <td>5.942173</td>\n",
              "      <td>0.001951</td>\n",
              "      <td>0.002309</td>\n",
              "      <td>7.182164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1.532607</td>\n",
              "      <td>8.155319</td>\n",
              "      <td>1.857537</td>\n",
              "      <td>4.977973</td>\n",
              "      <td>0.131614</td>\n",
              "      <td>3.981660</td>\n",
              "      <td>0.824544</td>\n",
              "      <td>37.766106</td>\n",
              "      <td>0.062101</td>\n",
              "      <td>6.659392</td>\n",
              "      <td>0.000902</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>6.309671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1.215913</td>\n",
              "      <td>6.036715</td>\n",
              "      <td>1.292110</td>\n",
              "      <td>4.627816</td>\n",
              "      <td>-0.059822</td>\n",
              "      <td>4.005053</td>\n",
              "      <td>0.803679</td>\n",
              "      <td>37.764408</td>\n",
              "      <td>0.077502</td>\n",
              "      <td>7.717176</td>\n",
              "      <td>-0.000535</td>\n",
              "      <td>-0.004452</td>\n",
              "      <td>4.881863</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1.428284</td>\n",
              "      <td>7.393489</td>\n",
              "      <td>1.670106</td>\n",
              "      <td>4.789704</td>\n",
              "      <td>0.087844</td>\n",
              "      <td>3.984734</td>\n",
              "      <td>0.801627</td>\n",
              "      <td>37.760178</td>\n",
              "      <td>0.067059</td>\n",
              "      <td>7.053225</td>\n",
              "      <td>0.000274</td>\n",
              "      <td>-0.002043</td>\n",
              "      <td>5.709802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1.507858</td>\n",
              "      <td>8.245743</td>\n",
              "      <td>1.771596</td>\n",
              "      <td>4.798844</td>\n",
              "      <td>0.105075</td>\n",
              "      <td>3.987632</td>\n",
              "      <td>0.823624</td>\n",
              "      <td>37.776489</td>\n",
              "      <td>0.060884</td>\n",
              "      <td>6.609706</td>\n",
              "      <td>0.000875</td>\n",
              "      <td>-0.000265</td>\n",
              "      <td>6.354975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1.444571</td>\n",
              "      <td>7.715159</td>\n",
              "      <td>1.752006</td>\n",
              "      <td>4.942852</td>\n",
              "      <td>0.073823</td>\n",
              "      <td>3.989223</td>\n",
              "      <td>0.820988</td>\n",
              "      <td>37.771343</td>\n",
              "      <td>0.065057</td>\n",
              "      <td>6.931331</td>\n",
              "      <td>0.000495</td>\n",
              "      <td>-0.001088</td>\n",
              "      <td>6.041512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1.596004</td>\n",
              "      <td>8.381153</td>\n",
              "      <td>1.924903</td>\n",
              "      <td>4.919916</td>\n",
              "      <td>0.187732</td>\n",
              "      <td>3.976864</td>\n",
              "      <td>0.818065</td>\n",
              "      <td>37.769279</td>\n",
              "      <td>0.058846</td>\n",
              "      <td>6.565007</td>\n",
              "      <td>0.000859</td>\n",
              "      <td>-0.000313</td>\n",
              "      <td>6.427517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>1.547607</td>\n",
              "      <td>8.105597</td>\n",
              "      <td>1.896615</td>\n",
              "      <td>4.948829</td>\n",
              "      <td>0.148563</td>\n",
              "      <td>3.982853</td>\n",
              "      <td>0.811586</td>\n",
              "      <td>37.752171</td>\n",
              "      <td>0.065639</td>\n",
              "      <td>6.723283</td>\n",
              "      <td>0.000825</td>\n",
              "      <td>-0.000106</td>\n",
              "      <td>6.238220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1.327754</td>\n",
              "      <td>6.881887</td>\n",
              "      <td>1.547180</td>\n",
              "      <td>4.706021</td>\n",
              "      <td>-0.039806</td>\n",
              "      <td>3.998295</td>\n",
              "      <td>0.820642</td>\n",
              "      <td>37.776039</td>\n",
              "      <td>0.069653</td>\n",
              "      <td>7.315424</td>\n",
              "      <td>-0.000339</td>\n",
              "      <td>-0.002770</td>\n",
              "      <td>5.427362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>1.658981</td>\n",
              "      <td>8.703653</td>\n",
              "      <td>2.032413</td>\n",
              "      <td>5.019574</td>\n",
              "      <td>0.238714</td>\n",
              "      <td>3.974280</td>\n",
              "      <td>0.822121</td>\n",
              "      <td>37.773319</td>\n",
              "      <td>0.057719</td>\n",
              "      <td>6.357906</td>\n",
              "      <td>0.001426</td>\n",
              "      <td>0.000741</td>\n",
              "      <td>6.684265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>1.535589</td>\n",
              "      <td>8.144583</td>\n",
              "      <td>1.866128</td>\n",
              "      <td>4.896502</td>\n",
              "      <td>0.138019</td>\n",
              "      <td>3.981009</td>\n",
              "      <td>0.819202</td>\n",
              "      <td>37.766113</td>\n",
              "      <td>0.060621</td>\n",
              "      <td>6.744747</td>\n",
              "      <td>0.000929</td>\n",
              "      <td>-0.000434</td>\n",
              "      <td>6.262188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>1.620048</td>\n",
              "      <td>8.533098</td>\n",
              "      <td>1.962841</td>\n",
              "      <td>4.978528</td>\n",
              "      <td>0.220851</td>\n",
              "      <td>3.974055</td>\n",
              "      <td>0.812119</td>\n",
              "      <td>37.763378</td>\n",
              "      <td>0.062410</td>\n",
              "      <td>6.443366</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>6.523177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>1.563844</td>\n",
              "      <td>8.165095</td>\n",
              "      <td>1.874010</td>\n",
              "      <td>4.873460</td>\n",
              "      <td>0.134634</td>\n",
              "      <td>3.983613</td>\n",
              "      <td>0.814583</td>\n",
              "      <td>37.764343</td>\n",
              "      <td>0.064433</td>\n",
              "      <td>6.625413</td>\n",
              "      <td>0.000767</td>\n",
              "      <td>-0.000437</td>\n",
              "      <td>6.291001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>1.556168</td>\n",
              "      <td>8.106724</td>\n",
              "      <td>1.883439</td>\n",
              "      <td>4.951327</td>\n",
              "      <td>0.140807</td>\n",
              "      <td>3.979396</td>\n",
              "      <td>0.815840</td>\n",
              "      <td>37.752861</td>\n",
              "      <td>0.060888</td>\n",
              "      <td>6.739352</td>\n",
              "      <td>0.000553</td>\n",
              "      <td>-0.000409</td>\n",
              "      <td>6.206906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>1.206281</td>\n",
              "      <td>6.233585</td>\n",
              "      <td>1.290058</td>\n",
              "      <td>4.552123</td>\n",
              "      <td>-0.111248</td>\n",
              "      <td>4.009642</td>\n",
              "      <td>0.823622</td>\n",
              "      <td>37.791691</td>\n",
              "      <td>0.075162</td>\n",
              "      <td>7.662960</td>\n",
              "      <td>-0.000734</td>\n",
              "      <td>-0.003962</td>\n",
              "      <td>5.161536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>1.031549</td>\n",
              "      <td>4.965668</td>\n",
              "      <td>1.087230</td>\n",
              "      <td>4.631914</td>\n",
              "      <td>-0.166000</td>\n",
              "      <td>4.019632</td>\n",
              "      <td>0.816031</td>\n",
              "      <td>37.768597</td>\n",
              "      <td>0.078411</td>\n",
              "      <td>8.479372</td>\n",
              "      <td>-0.001736</td>\n",
              "      <td>-0.006068</td>\n",
              "      <td>4.295568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>1.457702</td>\n",
              "      <td>7.729302</td>\n",
              "      <td>1.753841</td>\n",
              "      <td>4.908520</td>\n",
              "      <td>0.081926</td>\n",
              "      <td>3.987932</td>\n",
              "      <td>0.821799</td>\n",
              "      <td>37.778866</td>\n",
              "      <td>0.062064</td>\n",
              "      <td>6.881732</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>-0.001362</td>\n",
              "      <td>6.046563</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Workclass  Education  ...  Capital Loss  Hours per week\n",
              "0    1.487170   7.725574  ...     -0.001699        5.952133\n",
              "1    1.713927   8.978780  ...      0.001573        6.886689\n",
              "2    0.938487   4.035961  ...     -0.007605        3.985677\n",
              "3    1.530607   7.986984  ...     -0.000561        6.197436\n",
              "4    1.713759   8.982026  ...      0.001590        6.938636\n",
              "5    1.730287   8.976460  ...      0.001370        6.820226\n",
              "6    1.595349   8.235085  ...     -0.000100        6.316930\n",
              "7    1.579782   8.321161  ...      0.000111        6.427892\n",
              "8    1.614259   8.551085  ...      0.000347        6.532008\n",
              "9    1.872945   9.661609  ...      0.003065        7.394825\n",
              "10   1.724890   9.011563  ...      0.001496        6.800772\n",
              "11   1.583834   8.280060  ...     -0.000004        6.282540\n",
              "12   0.951133   4.270750  ...     -0.007312        3.893327\n",
              "13   0.537444  -2.218637  ...     -0.012421        2.094334\n",
              "14   1.605082   8.440021  ...     -0.000010        6.563414\n",
              "15   1.465360   7.668321  ...     -0.001256        5.922861\n",
              "16   1.606985   8.414943  ...      0.000256        6.528314\n",
              "17   1.146590   5.621670  ...     -0.005674        4.643296\n",
              "18   1.001755   4.573864  ...     -0.007684        3.937124\n",
              "19   1.611728   8.431912  ...      0.000206        6.602500\n",
              "20   1.049513   5.023397  ...     -0.006330        4.393504\n",
              "21   1.427069   7.501954  ...     -0.001646        5.797344\n",
              "22   0.910712   4.028115  ...     -0.007954        3.705495\n",
              "23   1.577421   8.443593  ...     -0.000008        6.496691\n",
              "24   1.418356   7.473160  ...     -0.001437        5.829005\n",
              "25   1.291739   6.907618  ...     -0.003361        5.488407\n",
              "26   1.773343   9.287158  ...      0.002123        7.124039\n",
              "27   1.818380   9.419474  ...      0.002574        7.219224\n",
              "28   1.439776   7.426140  ...     -0.002033        5.832234\n",
              "29   1.568415   8.339447  ...     -0.000137        6.385326\n",
              "30   1.380296   7.256145  ...     -0.002516        5.591858\n",
              "31   1.756653   9.139848  ...      0.001967        7.068528\n",
              "32   1.619518   8.483880  ...      0.000131        6.503043\n",
              "33   1.792905   9.419403  ...      0.002309        7.182164\n",
              "34   1.532607   8.155319  ...      0.000152        6.309671\n",
              "35   1.215913   6.036715  ...     -0.004452        4.881863\n",
              "36   1.428284   7.393489  ...     -0.002043        5.709802\n",
              "37   1.507858   8.245743  ...     -0.000265        6.354975\n",
              "38   1.444571   7.715159  ...     -0.001088        6.041512\n",
              "39   1.596004   8.381153  ...     -0.000313        6.427517\n",
              "40   1.547607   8.105597  ...     -0.000106        6.238220\n",
              "41   1.327754   6.881887  ...     -0.002770        5.427362\n",
              "42   1.658981   8.703653  ...      0.000741        6.684265\n",
              "43   1.535589   8.144583  ...     -0.000434        6.262188\n",
              "44   1.620048   8.533098  ...      0.000389        6.523177\n",
              "45   1.563844   8.165095  ...     -0.000437        6.291001\n",
              "46   1.556168   8.106724  ...     -0.000409        6.206906\n",
              "47   1.206281   6.233585  ...     -0.003962        5.161536\n",
              "48   1.031549   4.965668  ...     -0.006068        4.295568\n",
              "49   1.457702   7.729302  ...     -0.001362        6.046563\n",
              "\n",
              "[50 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL0YbAaCIho4"
      },
      "source": [
        "Now let's try to do a feature by feature comparision between the generated data and the actual data. We will use python's `table_evaluator` library to compare the features. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZwcPQPE00lq"
      },
      "source": [
        "!pip install table_evaluator\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVKBC6Ce17e_"
      },
      "source": [
        "#gen_samples.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "print(gen_samples.columns)\n",
        "#print(df.shape, gen_df.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z691TmXEIzzA"
      },
      "source": [
        "We call the `visual_evaluation` method to compare the actual date(`df`) and the generated data(`gen_df`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DON2kMjZ05ZN"
      },
      "source": [
        "from table_evaluator import load_data, TableEvaluator\n",
        "\n",
        "print(len(df), len(gen_samples))\n",
        "#print(fake.columns, real.columns)\n",
        "table_evaluator =  TableEvaluator(df, gen_samples)\n",
        "\n",
        "table_evaluator.visual_evaluation()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMzPe5GDI9mX"
      },
      "source": [
        " ## Conclusion\n",
        "\n",
        "Some of the features in the syntehtic data match closely with actual data but there are some other features which were not learnt perfectly by the model. We can keep playing with the model and its hyperparameters to improve the model further. \n",
        "\n",
        "This post demonstrates that its fairly simply to use GANs to generate synthetic data where the actual data is sensitive in nature and can't be shared publicly. "
      ]
    }
  ]
}